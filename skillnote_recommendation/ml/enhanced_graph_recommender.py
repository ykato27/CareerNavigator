"""
Enhanced Graph-based Recommender

æ”¹è‰¯ã•ã‚ŒãŸã‚°ãƒ©ãƒ•ãƒ™ãƒ¼ã‚¹æ¨è–¦ãƒ¢ãƒ‡ãƒ«

ä¸»ãªæ”¹å–„ç‚¹:
1. æ™‚é–“æ¸›è¡°é‡ã¿ä»˜ã‘: æœ€è¿‘ã®é·ç§»ã‚’é‡è¦–
2. ãƒ‘ã‚¹å“è³ªè©•ä¾¡: é•·ã•ãƒ»å¼·åº¦ãƒ»æ–°è¦æ€§ã‚’ç·åˆè©•ä¾¡
3. åŠ¹ç‡çš„ãªãƒ‘ã‚¹æ¢ç´¢: é‡è¦åº¦ãƒ™ãƒ¼ã‚¹ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°
4. Robust Scaling: å¤–ã‚Œå€¤ã«å¼·ã„æ­£è¦åŒ–
5. è©³ç´°ãªçµ±è¨ˆæƒ…å ±ã®ä¿å­˜
"""

import logging
from typing import List, Dict, Any, Optional, Tuple
import pandas as pd
import numpy as np
import networkx as nx
from node2vec import Node2Vec
from datetime import datetime, timedelta
from scipy.stats import rankdata

from skillnote_recommendation.ml.base_recommender import BaseRecommender, Recommendation

logger = logging.getLogger(__name__)


class EnhancedSkillTransitionGraphRecommender(BaseRecommender):
    """
    æ”¹è‰¯ç‰ˆã‚¹ã‚­ãƒ«é·ç§»ã‚°ãƒ©ãƒ•ãƒ™ãƒ¼ã‚¹ã®æ¨è–¦ãƒ¢ãƒ‡ãƒ«

    ä¸»ãªæ”¹å–„:
    - æ™‚é–“æ¸›è¡°é‡ã¿ä»˜ã‘ï¼ˆæŒ‡æ•°æ¸›è¡°ï¼‰
    - ãƒ‘ã‚¹å“è³ªã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
    - Robust Scaling ã«ã‚ˆã‚‹æ­£è¦åŒ–
    - å‹•çš„é‡ã¿èª¿æ•´
    """

    def __init__(
        self,
        time_window_days: int = 180,
        min_transition_count: int = 2,
        embedding_dim: int = 64,
        walk_length: int = 10,
        num_walks: int = 80,
        p: float = 1.0,
        q: float = 2.0,
        workers: int = 4,
        time_decay_factor: float = 0.01,  # æ™‚é–“æ¸›è¡°ä¿‚æ•°ï¼ˆ1æ—¥ã‚ãŸã‚Šï¼‰
        use_time_decay: bool = True,
        path_quality_weight: float = 0.3,  # ãƒ‘ã‚¹å“è³ªã®é‡ã¿
        use_robust_scaling: bool = True
    ):
        """
        åˆæœŸåŒ–

        Args:
            time_window_days: é·ç§»ã¨ã¿ãªã™æœ€å¤§æœŸé–“ï¼ˆæ—¥æ•°ï¼‰
            min_transition_count: æœ€å°é·ç§»äººæ•°
            embedding_dim: åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒæ•°
            walk_length: ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ã®é•·ã•
            num_walks: ã‚¦ã‚©ãƒ¼ã‚¯å›æ•°
            p: Return parameterï¼ˆDFS vs BFSï¼‰
            q: In-out parameterï¼ˆlocal vs globalï¼‰
            workers: ä¸¦åˆ—å‡¦ç†æ•°
            time_decay_factor: æ™‚é–“æ¸›è¡°ä¿‚æ•°ï¼ˆ0.01 = 1æ—¥ã‚ãŸã‚Š1%æ¸›è¡°ï¼‰
            use_time_decay: æ™‚é–“æ¸›è¡°é‡ã¿ä»˜ã‘ã‚’ä½¿ç”¨
            path_quality_weight: ãƒ‘ã‚¹å“è³ªã‚¹ã‚³ã‚¢ã®é‡ã¿
            use_robust_scaling: Robust Scalingã‚’ä½¿ç”¨ï¼ˆå¤–ã‚Œå€¤ã«å¼·ã„ï¼‰
        """
        super().__init__(
            name="EnhancedSkillTransitionGraph",
            interpretability_score=5  # æ”¹å–„ã«ã‚ˆã‚Šè§£é‡ˆæ€§ãŒå‘ä¸Š
        )

        self.time_window_days = time_window_days
        self.min_transition_count = min_transition_count
        self.embedding_dim = embedding_dim
        self.walk_length = walk_length
        self.num_walks = num_walks
        self.p = p
        self.q = q
        self.workers = workers
        self.time_decay_factor = time_decay_factor
        self.use_time_decay = use_time_decay
        self.path_quality_weight = path_quality_weight
        self.use_robust_scaling = use_robust_scaling

        self.graph = None
        self.node2vec_model = None
        self.transition_stats = {}
        self.edge_details = {}  # è©³ç´°ãªã‚¨ãƒƒã‚¸æƒ…å ±

    def fit(self, member_competence: pd.DataFrame, competence_master: pd.DataFrame) -> None:
        """
        ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’

        Args:
            member_competence: ãƒ¡ãƒ³ãƒãƒ¼ç¿’å¾—åŠ›é‡ãƒ‡ãƒ¼ã‚¿
            competence_master: åŠ›é‡ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿
        """
        logger.info("=" * 80)
        logger.info("EnhancedSkillTransitionGraphRecommender ã®å­¦ç¿’ã‚’é–‹å§‹")
        logger.info("=" * 80)

        self.member_competence = member_competence.copy()
        self.competence_master = competence_master.copy()

        # å–å¾—æ—¥ã‚«ãƒ©ãƒ ã®ç¢ºèª
        if 'å–å¾—æ—¥' not in member_competence.columns:
            raise ValueError("ã‚¹ã‚­ãƒ«é·ç§»ã‚°ãƒ©ãƒ•ã«ã¯ã€Œå–å¾—æ—¥ã€ã‚«ãƒ©ãƒ ãŒå¿…è¦ã§ã™")

        # ã‚°ãƒ©ãƒ•æ§‹ç¯‰
        logger.info("\nStep 1: æ”¹è‰¯ç‰ˆã‚¹ã‚­ãƒ«é·ç§»ã‚°ãƒ©ãƒ•ã®æ§‹ç¯‰")
        self.graph = self._build_enhanced_transition_graph()
        logger.info(f"  ãƒãƒ¼ãƒ‰æ•°: {self.graph.number_of_nodes()}")
        logger.info(f"  ã‚¨ãƒƒã‚¸æ•°: {self.graph.number_of_edges()}")
        logger.info(f"  æ™‚é–“æ¸›è¡°: {'æœ‰åŠ¹' if self.use_time_decay else 'ç„¡åŠ¹'}")

        # Node2Vecå­¦ç¿’
        if self.graph.number_of_nodes() > 1:
            logger.info("\nStep 2: Node2VecåŸ‹ã‚è¾¼ã¿ã®å­¦ç¿’")
            self._train_node2vec()
            logger.info(f"  åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ: {self.embedding_dim}")

        self.is_fitted = True
        self.metadata = {
            'num_nodes': self.graph.number_of_nodes(),
            'num_edges': self.graph.number_of_edges(),
            'time_window_days': self.time_window_days,
            'time_decay_enabled': self.use_time_decay,
            'has_embeddings': self.node2vec_model is not None
        }

        logger.info("\n" + "=" * 80)
        logger.info("å­¦ç¿’å®Œäº†")
        logger.info("=" * 80)

    def _build_enhanced_transition_graph(self) -> nx.DiGraph:
        """
        æ”¹è‰¯ç‰ˆã‚¹ã‚­ãƒ«é·ç§»ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰

        æ”¹å–„ç‚¹:
        - æ™‚é–“æ¸›è¡°é‡ã¿ä»˜ã‘
        - è©³ç´°ãªçµ±è¨ˆæƒ…å ±ã®ä¿å­˜
        - åŠ¹ç‡çš„ãªè¨ˆç®—ï¼ˆé€£ç¶šãƒšã‚¢ã®ã¿ï¼‰

        Returns:
            æœ‰å‘ã‚°ãƒ©ãƒ•
        """
        G = nx.DiGraph()

        # å–å¾—æ—¥ã‚’æ—¥ä»˜å‹ã«å¤‰æ›
        df = self.member_competence.copy()
        df['å–å¾—æ—¥_dt'] = pd.to_datetime(df['å–å¾—æ—¥'], errors='coerce')
        df = df[df['å–å¾—æ—¥_dt'].notna()]

        # ç¾åœ¨æ—¥æ™‚ï¼ˆæ™‚é–“æ¸›è¡°ã®åŸºæº–ç‚¹ï¼‰
        now = pd.Timestamp.now()

        # ãƒ¡ãƒ³ãƒãƒ¼ã”ã¨ã«å­¦ç¿’é †åºã‚’æŠ½å‡º
        transition_data = {}

        for member in df['ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰'].unique():
            member_skills = df[
                df['ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰'] == member
            ].sort_values('å–å¾—æ—¥_dt')

            skills = member_skills[['åŠ›é‡ã‚³ãƒ¼ãƒ‰', 'å–å¾—æ—¥_dt']].values

            # é€£ç¶šã™ã‚‹ã‚¹ã‚­ãƒ«ãƒšã‚¢ã®ã¿ã‚’æŠ½å‡ºï¼ˆåŠ¹ç‡åŒ–ï¼‰
            for i in range(len(skills) - 1):
                source_skill, source_date = skills[i]
                target_skill, target_date = skills[i + 1]

                # æ™‚é–“çª“å†…ã®é·ç§»ã®ã¿
                time_diff = (target_date - source_date).days
                if time_diff <= self.time_window_days:
                    edge = (source_skill, target_skill)

                    if edge not in transition_data:
                        transition_data[edge] = {
                            'transitions': [],
                            'acquisition_dates': []
                        }

                    # æ™‚é–“æ¸›è¡°é‡ã¿ã‚’è¨ˆç®—
                    days_ago = (now - target_date).days
                    if self.use_time_decay:
                        decay_weight = np.exp(-self.time_decay_factor * days_ago / 365)
                    else:
                        decay_weight = 1.0

                    transition_data[edge]['transitions'].append({
                        'time_diff': time_diff,
                        'decay_weight': decay_weight,
                        'acquisition_date': target_date
                    })
                    transition_data[edge]['acquisition_dates'].append(target_date)

        # ã‚¨ãƒƒã‚¸ã‚’è¿½åŠ 
        for (source, target), data in transition_data.items():
            transitions = data['transitions']
            count = len(transitions)

            if count >= self.min_transition_count:
                # æ™‚é–“æ¸›è¡°é‡ã¿ä»˜ãã‚«ã‚¦ãƒ³ãƒˆ
                weighted_count = sum(t['decay_weight'] for t in transitions)

                # çµ±è¨ˆæƒ…å ±ã‚’è¨ˆç®—
                time_diffs = [t['time_diff'] for t in transitions]
                avg_time_diff = np.mean(time_diffs)
                median_time_diff = np.median(time_diffs)
                std_time_diff = np.std(time_diffs)

                # æœ€æ–°ã®é·ç§»æ—¥
                latest_transition = max(data['acquisition_dates'])
                recency_days = (now - latest_transition).days

                # ã‚¨ãƒƒã‚¸ã‚’è¿½åŠ 
                G.add_edge(
                    source,
                    target,
                    weight=weighted_count,
                    raw_count=count,
                    avg_time_diff=avg_time_diff,
                    median_time_diff=median_time_diff,
                    std_time_diff=std_time_diff,
                    recency_days=recency_days,
                )

                # è©³ç´°æƒ…å ±ã‚’ä¿å­˜
                self.edge_details[(source, target)] = {
                    'count': count,
                    'weighted_count': weighted_count,
                    'avg_days': avg_time_diff,
                    'median_days': median_time_diff,
                    'std_days': std_time_diff,
                    'recency_days': recency_days,
                    'transitions': transitions
                }

        return G

    def _train_node2vec(self) -> None:
        """Node2Vecã§åŸ‹ã‚è¾¼ã¿ã‚’å­¦ç¿’"""
        try:
            node2vec = Node2Vec(
                self.graph,
                dimensions=self.embedding_dim,
                walk_length=self.walk_length,
                num_walks=self.num_walks,
                p=self.p,
                q=self.q,
                workers=self.workers,
                quiet=True
            )

            self.node2vec_model = node2vec.fit(
                window=5,
                min_count=1,
                batch_words=4,
                epochs=5
            )

        except Exception as e:
            logger.error(f"Node2Vecå­¦ç¿’ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            self.node2vec_model = None

    def recommend(
        self,
        member_code: str,
        n: int = 10,
        exclude_acquired: bool = True,
        competence_type: Optional[List[str]] = None
    ) -> List[Recommendation]:
        """
        æ¨è–¦ãƒªã‚¹ãƒˆã®ç”Ÿæˆï¼ˆæ”¹è‰¯ç‰ˆï¼‰

        Args:
            member_code: ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰
            n: æ¨è–¦ã™ã‚‹ä»¶æ•°
            exclude_acquired: æ—¢ç¿’å¾—ã‚¹ã‚­ãƒ«ã‚’é™¤å¤–ã™ã‚‹ã‹
            competence_type: ãƒ•ã‚£ãƒ«ã‚¿ã™ã‚‹åŠ›é‡ã‚¿ã‚¤ãƒ—ã®ãƒªã‚¹ãƒˆï¼ˆä¾‹: ['SKILL', 'EDUCATION']ï¼‰
                             Noneã®å ´åˆã¯å…¨ã¦ã®åŠ›é‡ã‚¿ã‚¤ãƒ—ã‚’æ¨è–¦

        Returns:
            æ¨è–¦çµæœã®ãƒªã‚¹ãƒˆ
        """
        self._check_fitted()

        user_skills = self.get_user_skills(member_code)

        if not user_skills:
            logger.warning(f"ãƒ¡ãƒ³ãƒãƒ¼ {member_code} ã¯ç¿’å¾—ã‚¹ã‚­ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
            return []

        # å€™è£œã‚¹ã‚­ãƒ«ã‚’åé›†
        candidates = {}

        # æ–¹æ³•1: ã‚°ãƒ©ãƒ•ã®éš£æ¥ãƒãƒ¼ãƒ‰ï¼ˆæ™‚é–“æ¸›è¡°é‡ã¿ä»˜ãï¼‰
        for skill in user_skills:
            if skill in self.graph:
                for neighbor in self.graph.neighbors(skill):
                    if exclude_acquired and neighbor in user_skills:
                        continue

                    edge_data = self.graph[skill][neighbor]
                    weighted_count = edge_data['weight']

                    # ãƒ‘ã‚¹å“è³ªã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
                    path_quality = self._calculate_path_quality(
                        source=skill,
                        target=neighbor,
                        path_length=2
                    )

                    # ç·åˆã‚¹ã‚³ã‚¢
                    score = weighted_count * (1 + self.path_quality_weight * path_quality)

                    if neighbor not in candidates or candidates[neighbor]['score'] < score:
                        candidates[neighbor] = {
                            'score': score,
                            'source_skill': skill,
                            'reason_type': 'direct_transition',
                            'metadata': edge_data,
                            'path_quality': path_quality
                        }

        # æ–¹æ³•2: åŸ‹ã‚è¾¼ã¿ç©ºé–“ã§ã®é¡ä¼¼åº¦ï¼ˆRobust Scalingï¼‰
        if self.node2vec_model is not None:
            embedding_scores = self._get_embedding_recommendations(
                user_skills, exclude_acquired, n * 2
            )

            for skill, score in embedding_scores.items():
                if skill in candidates:
                    # æ—¢å­˜ã‚¹ã‚³ã‚¢ã¨ãƒ–ãƒ¬ãƒ³ãƒ‰
                    candidates[skill]['score'] += score
                    candidates[skill]['has_embedding_similarity'] = True
                else:
                    candidates[skill] = {
                        'score': score,
                        'source_skill': user_skills[0],
                        'reason_type': 'embedding_similarity',
                        'metadata': {},
                        'path_quality': 0.5
                    }

        # åŠ›é‡ã‚¿ã‚¤ãƒ—ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        if competence_type is not None:
            logger.info(f"åŠ›é‡ã‚¿ã‚¤ãƒ—ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨é–‹å§‹: {competence_type}")
            logger.info(f"ãƒ•ã‚£ãƒ«ã‚¿å‰ã®å€™è£œæ•°: {len(candidates)}")

            filtered_candidates = {}
            rejected_count = 0
            for skill_code, data in candidates.items():
                skill_type = self._get_skill_type(skill_code)
                skill_name = self.get_skill_name(skill_code)

                if skill_type in competence_type:
                    filtered_candidates[skill_code] = data
                    logger.debug(f"  âœ“ {skill_name} ({skill_code}) - ã‚¿ã‚¤ãƒ—: {skill_type} -> æ¡ç”¨")
                else:
                    rejected_count += 1
                    logger.debug(f"  âœ— {skill_name} ({skill_code}) - ã‚¿ã‚¤ãƒ—: {skill_type} -> é™¤å¤–")

            candidates = filtered_candidates
            logger.info(f"ãƒ•ã‚£ãƒ«ã‚¿å¾Œã®å€™è£œæ•°: {len(candidates)} (é™¤å¤–: {rejected_count}ä»¶)")

        # ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
        sorted_candidates = sorted(
            candidates.items(),
            key=lambda x: x[1]['score'],
            reverse=True
        )[:n]

        # Recommendationã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
        recommendations = []
        for rank, (skill_code, data) in enumerate(sorted_candidates, 1):
            skill_name = self.get_skill_name(skill_code)
            explanation = self.explain(member_code, skill_code)

            # ä¿¡é ¼åº¦ã®è¨ˆç®—ï¼ˆãƒ‘ã‚¹å“è³ªã‚’è€ƒæ…®ï¼‰
            confidence = self._calculate_confidence(data)

            recommendations.append(Recommendation(
                skill_code=skill_code,
                skill_name=skill_name,
                score=data['score'],
                rank=rank,
                explanation=explanation,
                confidence=confidence,
                metadata=data
            ))

        return recommendations

    def _calculate_path_quality(self,
                                source: str,
                                target: str,
                                path_length: int) -> float:
        """
        ãƒ‘ã‚¹å“è³ªã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆ0-1ï¼‰

        è€ƒæ…®è¦ç´ :
        - ãƒ‘ã‚¹é•·ï¼ˆçŸ­ã„ã»ã©è‰¯ã„ï¼‰
        - é·ç§»å¼·åº¦ï¼ˆå¤šã„ã»ã©è‰¯ã„ï¼‰
        - é·ç§»ã®å®‰å®šæ€§ï¼ˆæ¨™æº–åå·®ãŒå°ã•ã„ã»ã©è‰¯ã„ï¼‰
        - æ–°è¦æ€§ï¼ˆæœ€è¿‘ã®é·ç§»ãŒã‚ã‚‹ã»ã©è‰¯ã„ï¼‰

        Returns:
            å“è³ªã‚¹ã‚³ã‚¢ï¼ˆ0-1ï¼‰
        """
        if (source, target) not in self.edge_details:
            return 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

        edge_info = self.edge_details[(source, target)]

        # ãƒ‘ã‚¹é•·ã‚¹ã‚³ã‚¢ï¼ˆ2ãŒæœ€é©ã€é•·ã„ã»ã©æ¸›ç‚¹ï¼‰
        if path_length <= 2:
            length_score = 1.0
        else:
            length_score = max(0.3, 1.0 - (path_length - 2) * 0.15)

        # é·ç§»å¼·åº¦ã‚¹ã‚³ã‚¢ï¼ˆå¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
        count = edge_info['count']
        strength_score = min(1.0, np.log1p(count) / np.log1p(50))

        # å®‰å®šæ€§ã‚¹ã‚³ã‚¢ï¼ˆæ¨™æº–åå·®ãŒå°ã•ã„ã»ã©è‰¯ã„ï¼‰
        std_days = edge_info['std_days']
        if std_days <= 30:
            stability_score = 1.0
        else:
            stability_score = max(0.3, 1.0 - (std_days - 30) / 180)

        # æ–°è¦æ€§ã‚¹ã‚³ã‚¢ï¼ˆæœ€è¿‘ã®é·ç§»ãŒã‚ã‚‹ã»ã©è‰¯ã„ï¼‰
        recency_days = edge_info['recency_days']
        if recency_days <= 90:
            recency_score = 1.0
        elif recency_days <= 365:
            recency_score = 0.8
        else:
            recency_score = max(0.3, 1.0 - (recency_days - 365) / 730)

        # é‡ã¿ä»˜ãå¹³å‡
        quality = (
            0.25 * length_score +
            0.30 * strength_score +
            0.25 * stability_score +
            0.20 * recency_score
        )

        return quality

    def _get_embedding_recommendations(self,
                                      user_skills: List[str],
                                      exclude_acquired: bool,
                                      n: int) -> Dict[str, float]:
        """
        åŸ‹ã‚è¾¼ã¿ç©ºé–“ã§ã®æ¨è–¦ï¼ˆRobust Scalingé©ç”¨ï¼‰

        Returns:
            {skill_code: score}
        """
        all_similarities = {}

        for skill in user_skills:
            if skill not in self.node2vec_model.wv:
                continue

            try:
                similar_skills = self.node2vec_model.wv.most_similar(skill, topn=n)
                for sim_skill, similarity in similar_skills:
                    if exclude_acquired and sim_skill in user_skills:
                        continue

                    if sim_skill not in all_similarities:
                        all_similarities[sim_skill] = []
                    all_similarities[sim_skill].append(similarity)
            except Exception as e:
                logger.warning(f"é¡ä¼¼ã‚¹ã‚­ãƒ«å–å¾—ã‚¨ãƒ©ãƒ¼ ({skill}): {e}")

        # å¹³å‡é¡ä¼¼åº¦ã‚’è¨ˆç®—
        avg_similarities = {
            skill: np.mean(sims) for skill, sims in all_similarities.items()
        }

        # Robust Scalingï¼ˆå¤–ã‚Œå€¤ã«å¼·ã„ï¼‰
        if self.use_robust_scaling and avg_similarities:
            scores = list(avg_similarities.values())
            median = np.median(scores)
            q75, q25 = np.percentile(scores, [75, 25])
            iqr = q75 - q25

            if iqr > 0:
                scaled = {
                    skill: ((score - median) / iqr) * 5 + 5  # 0-10ã®ã‚¹ã‚±ãƒ¼ãƒ«
                    for skill, score in avg_similarities.items()
                }
                return {k: max(0, v) for k, v in scaled.items()}

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ãã®ã¾ã¾10å€
        return {k: v * 10 for k, v in avg_similarities.items()}

    def _calculate_confidence(self, data: Dict) -> float:
        """ä¿¡é ¼åº¦ã‚’è¨ˆç®—"""
        if data['reason_type'] == 'direct_transition':
            # é·ç§»äººæ•°ã¨ãƒ‘ã‚¹å“è³ªã‹ã‚‰è¨ˆç®—
            raw_count = data['metadata'].get('raw_count', 0)
            path_quality = data.get('path_quality', 0.5)

            count_confidence = min(1.0, raw_count / 10)
            confidence = 0.6 * count_confidence + 0.4 * path_quality
        else:
            # åŸ‹ã‚è¾¼ã¿é¡ä¼¼åº¦ã‹ã‚‰è¨ˆç®—
            confidence = 0.6
            if 'has_embedding_similarity' in data:
                confidence = 0.7

        return confidence

    def explain(self, member_code: str, skill_code: str) -> str:
        """æ¨è–¦ç†ç”±ã‚’èª¬æ˜"""
        self._check_fitted()

        user_skills = self.get_user_skills(member_code)
        skill_name = self.get_skill_name(skill_code)

        explanations = []

        # 1. ç›´æ¥é·ç§»ã®èª¬æ˜ï¼ˆè©³ç´°ç‰ˆï¼‰
        for user_skill in user_skills:
            if self.graph.has_edge(user_skill, skill_code):
                source_name = self.get_skill_name(user_skill)

                if (user_skill, skill_code) in self.edge_details:
                    details = self.edge_details[(user_skill, skill_code)]
                    count = details['count']
                    median_days = details['median_days']
                    recency_days = details['recency_days']

                    recency_text = ""
                    if recency_days <= 90:
                        recency_text = "ï¼ˆæœ€è¿‘ã®é·ç§»å®Ÿç¸¾ã‚ã‚Šï¼‰"

                    explanations.append(
                        f"ğŸ¯ {source_name}ã‚’ç¿’å¾—ã—ãŸ{count}äººãŒæ¬¡ã«{skill_name}ã‚’å­¦ç¿’ "
                        f"ï¼ˆå¹³å‡{median_days:.0f}æ—¥å¾Œï¼‰{recency_text}"
                    )

        # 2. å­¦ç¿’ãƒ‘ã‚¹ã®èª¬æ˜
        for user_skill in user_skills:
            try:
                if user_skill in self.graph and skill_code in self.graph:
                    path = nx.shortest_path(self.graph, user_skill, skill_code)
                    if 2 <= len(path) <= 4:
                        path_names = [self.get_skill_name(s) for s in path]
                        path_str = " â†’ ".join(path_names)

                        # ãƒ‘ã‚¹å“è³ªã‚’è¨ˆç®—
                        quality = self._calculate_path_quality(
                            user_skill, skill_code, len(path)
                        )
                        quality_text = ""
                        if quality >= 0.7:
                            quality_text = "ï¼ˆé«˜å“è³ªãƒ‘ã‚¹ï¼‰"

                        explanations.append(f"ğŸ“š æ¨å¥¨å­¦ç¿’ãƒ‘ã‚¹: {path_str} {quality_text}")
                        break
            except (nx.NetworkXNoPath, nx.NodeNotFound):
                continue

        if not explanations:
            return f"ã‚°ãƒ©ãƒ•æ§‹é€ ã‹ã‚‰{skill_name}ãŒæ¨è–¦ã•ã‚Œã¾ã—ãŸ"

        return "\n".join(explanations)

    def get_edge_statistics(self) -> Dict[Tuple[str, str], Dict[str, Any]]:
        """ã‚¨ãƒƒã‚¸ã®è©³ç´°çµ±è¨ˆã‚’å–å¾—"""
        return self.edge_details.copy()

    def _get_skill_type(self, skill_code: str) -> str:
        """
        ã‚¹ã‚­ãƒ«ã®åŠ›é‡ã‚¿ã‚¤ãƒ—ã‚’å–å¾—

        Args:
            skill_code: ã‚¹ã‚­ãƒ«ã‚³ãƒ¼ãƒ‰

        Returns:
            åŠ›é‡ã‚¿ã‚¤ãƒ—ï¼ˆ'SKILL', 'EDUCATION', 'LICENSE', 'UNKNOWN'ï¼‰
        """
        if self.competence_master is None:
            return 'UNKNOWN'

        # åŠ›é‡ã‚³ãƒ¼ãƒ‰ã®æ¤œç´¢ï¼ˆæ—¥æœ¬èªãƒ»è‹±èªä¸¡å¯¾å¿œï¼‰
        code_col = 'åŠ›é‡ã‚³ãƒ¼ãƒ‰' if 'åŠ›é‡ã‚³ãƒ¼ãƒ‰' in self.competence_master.columns else 'competence_code'
        skill_row = self.competence_master[
            self.competence_master[code_col] == skill_code
        ]

        if skill_row.empty:
            return 'UNKNOWN'

        # åŠ›é‡ã‚¿ã‚¤ãƒ—ã‚«ãƒ©ãƒ ã®æ¤œç´¢ï¼ˆæ—¥æœ¬èªãƒ»è‹±èªä¸¡å¯¾å¿œï¼‰
        if 'åŠ›é‡ã‚¿ã‚¤ãƒ—' in skill_row.columns:
            return skill_row.iloc[0]['åŠ›é‡ã‚¿ã‚¤ãƒ—']
        elif 'competence_type' in skill_row.columns:
            return skill_row.iloc[0]['competence_type']
        else:
            return 'UNKNOWN'
