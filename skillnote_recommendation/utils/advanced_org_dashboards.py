"""
é«˜åº¦ãªçµ„ç¹”åˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ

ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆè¦–ç‚¹ã§ã®æˆ¦ç•¥çš„äººæåˆ†ææ©Ÿèƒ½ã‚’æä¾›
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from typing import Dict, Optional, List, Tuple
from scipy import stats


def extract_category_hierarchy(category_name: str, level: int = 1) -> str:
    """
    ã‚«ãƒ†ã‚´ãƒªåã‹ã‚‰æŒ‡å®šéšå±¤ã¾ã§ã‚’æŠ½å‡ºï¼ˆãƒ•ãƒ«ãƒ‘ã‚¹ä¿æŒï¼‰

    Args:
        category_name: ã‚«ãƒ†ã‚´ãƒªåï¼ˆä¾‹: "æŠ€è¡“ > ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚° > Python"ï¼‰
        level: æŠ½å‡ºã™ã‚‹éšå±¤ãƒ¬ãƒ™ãƒ«ï¼ˆ1=ç¬¬ä¸€éšå±¤ã€2=ç¬¬äºŒéšå±¤ã€3=ç¬¬ä¸‰éšå±¤ï¼‰

    Returns:
        æŒ‡å®šéšå±¤ã¾ã§ã®ã‚«ãƒ†ã‚´ãƒªåï¼ˆä¾‹: "æŠ€è¡“ > ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°"ï¼‰
    """
    if pd.isna(category_name):
        return "æœªåˆ†é¡"

    parts = str(category_name).split(" > ")
    if level > len(parts):
        return " > ".join(parts)

    return " > ".join(parts[:level])


def format_category_for_display(category_path: str) -> str:
    """
    ã‚«ãƒ†ã‚´ãƒªãƒ‘ã‚¹ã‚’éšå±¤çš„ã«è¡¨ç¤ºã™ã‚‹ãŸã‚ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ

    Args:
        category_path: ã‚«ãƒ†ã‚´ãƒªãƒ‘ã‚¹ï¼ˆä¾‹: "æŠ€è¡“ > ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚° > Python"ï¼‰

    Returns:
        éšå±¤ã«å¿œã˜ã¦ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆä»˜ãã®æœ€çµ‚éšå±¤å
    """
    if pd.isna(category_path) or category_path == "æœªåˆ†é¡":
        return "æœªåˆ†é¡"

    parts = str(category_path).split(" > ")
    level = len(parts)

    # éšå±¤ã«å¿œã˜ã¦ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆã‚’è¿½åŠ 
    indent = "  " * (level - 1)
    return indent + parts[-1]  # æœ€å¾Œã®éšå±¤ã®ã¿è¡¨ç¤º


def render_hierarchical_category_heatmap(
    member_competence_df: pd.DataFrame,
    competence_master_df: pd.DataFrame,
    members_df: pd.DataFrame,
    group_by: str = "è·ç¨®"
) -> None:
    """
    â‘ ã‚«ãƒ†ã‚´ãƒªÃ—è·ç¨®ã®éšå±¤çš„ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—

    ã‚«ãƒ†ã‚´ãƒªã‚’éšå±¤çš„ã«é¸æŠã§ãã€å¹³å‡å€¤/ä¸­å¤®å€¤ã‚’é¸æŠå¯èƒ½

    Args:
        member_competence_df: ãƒ¡ãƒ³ãƒãƒ¼åŠ›é‡ãƒãƒˆãƒªã‚¯ã‚¹
        competence_master_df: åŠ›é‡ãƒã‚¹ã‚¿
        members_df: ãƒ¡ãƒ³ãƒãƒ¼ãƒã‚¹ã‚¿
        group_by: ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã™ã‚‹è»¸ï¼ˆ"è·ç¨®", "å½¹è·"ç­‰ï¼‰
    """
    st.markdown(f"### ğŸ“Š ã‚«ãƒ†ã‚´ãƒªåˆ¥ Ã— {group_by}åˆ¥ ã‚¹ã‚­ãƒ«åˆ†æ")

    # ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãƒ‘ãƒãƒ«
    col1, col2, col3 = st.columns([2, 1, 1])

    with col1:
        hierarchy_level = st.selectbox(
            "ã‚«ãƒ†ã‚´ãƒªéšå±¤",
            options=[1, 2, 3],
            format_func=lambda x: ["ç¬¬ä¸€éšå±¤", "ç¬¬äºŒéšå±¤", "ç¬¬ä¸‰éšå±¤"][x-1],
            help="ã‚«ãƒ†ã‚´ãƒªã®è©³ç´°åº¦ã‚’é¸æŠã—ã¾ã™"
        )

    with col2:
        aggregation_method = st.selectbox(
            "é›†è¨ˆæ–¹æ³•",
            options=["mean", "median"],
            format_func=lambda x: "å¹³å‡å€¤" if x == "mean" else "ä¸­å¤®å€¤"
        )

    with col3:
        show_count = st.checkbox("äººæ•°ã‚‚è¡¨ç¤º", value=False)

    # ãƒ‡ãƒ¼ã‚¿æº–å‚™
    # åŠ›é‡ãƒã‚¹ã‚¿ã«ã‚«ãƒ†ã‚´ãƒªéšå±¤ã‚’è¿½åŠ 
    competence_master_df = competence_master_df.copy()
    if "åŠ›é‡ã‚«ãƒ†ã‚´ãƒªãƒ¼å" in competence_master_df.columns:
        competence_master_df["ã‚«ãƒ†ã‚´ãƒªéšå±¤"] = competence_master_df["åŠ›é‡ã‚«ãƒ†ã‚´ãƒªãƒ¼å"].apply(
            lambda x: extract_category_hierarchy(x, hierarchy_level)
        )
    else:
        st.warning("åŠ›é‡ã‚«ãƒ†ã‚´ãƒªãƒ¼åãŒãƒã‚¹ã‚¿ã«å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
        return

    # ãƒ¡ãƒ³ãƒãƒ¼åŠ›é‡ã«ã‚«ãƒ†ã‚´ãƒªéšå±¤ã‚’çµåˆ
    merged_df = member_competence_df.merge(
        competence_master_df[["åŠ›é‡ã‚³ãƒ¼ãƒ‰", "ã‚«ãƒ†ã‚´ãƒªéšå±¤"]],
        on="åŠ›é‡ã‚³ãƒ¼ãƒ‰",
        how="left"
    )

    # ãƒ¡ãƒ³ãƒãƒ¼æƒ…å ±ã‚’çµåˆ
    if group_by not in members_df.columns:
        st.warning(f"{group_by}æƒ…å ±ãŒãƒ¡ãƒ³ãƒãƒ¼ãƒã‚¹ã‚¿ã«å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
        return

    merged_df = merged_df.merge(
        members_df[["ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰", group_by]],
        on="ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰",
        how="left"
    )

    # ã‚«ãƒ†ã‚´ãƒªé¸æŠï¼ˆè¤‡æ•°é¸æŠå¯èƒ½ï¼‰
    available_categories = sorted(merged_df["ã‚«ãƒ†ã‚´ãƒªéšå±¤"].dropna().unique())

    selected_categories = st.multiselect(
        "è¡¨ç¤ºã™ã‚‹ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠ",
        options=available_categories,
        default=available_categories[:10] if len(available_categories) > 10 else available_categories,
        help="åˆ†æå¯¾è±¡ã®ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠã—ã¦ãã ã•ã„ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ä¸Šä½10ä»¶ï¼‰"
    )

    if not selected_categories:
        st.info("ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠã—ã¦ãã ã•ã„")
        return

    # é¸æŠã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªã®ãƒ‡ãƒ¼ã‚¿ã®ã¿æŠ½å‡º
    filtered_df = merged_df[merged_df["ã‚«ãƒ†ã‚´ãƒªéšå±¤"].isin(selected_categories)]

    # ä¿æœ‰é‡ã‚«ãƒ©ãƒ ã®æ¤œå‡º
    level_col = None
    for col in ["ä¿æœ‰é‡", "åŠ›é‡ãƒ¬ãƒ™ãƒ«", "ãƒ¬ãƒ™ãƒ«"]:
        if col in filtered_df.columns:
            level_col = col
            break

    if level_col is None:
        st.warning("ä¿æœ‰é‡ã¾ãŸã¯ãƒ¬ãƒ™ãƒ«æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return

    # ä¿æœ‰é‡ã‚’æ•°å€¤å‹ã«å¤‰æ›ï¼ˆã‚¨ãƒ©ãƒ¼å›é¿ï¼‰
    filtered_df[level_col] = pd.to_numeric(filtered_df[level_col], errors='coerce')

    # NaNã‚’é™¤å¤–
    filtered_df = filtered_df.dropna(subset=[level_col])

    if len(filtered_df) == 0:
        st.warning("æœ‰åŠ¹ãªä¿æœ‰é‡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return

    # é›†è¨ˆ
    if aggregation_method == "mean":
        pivot_df = filtered_df.groupby(["ã‚«ãƒ†ã‚´ãƒªéšå±¤", group_by])[level_col].mean().unstack(fill_value=0)
    else:
        pivot_df = filtered_df.groupby(["ã‚«ãƒ†ã‚´ãƒªéšå±¤", group_by])[level_col].median().unstack(fill_value=0)

    # ã‚«ãƒ†ã‚´ãƒªéšå±¤ã§ã‚½ãƒ¼ãƒˆï¼ˆéšå±¤çš„ãªé †åºã‚’ä¿æŒï¼‰
    pivot_df = pivot_df.sort_index()

    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’éšå±¤çš„ãªè¡¨ç¤ºã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
    formatted_index = [format_category_for_display(cat) for cat in pivot_df.index]
    pivot_df_display = pivot_df.copy()
    pivot_df_display.index = formatted_index

    # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—æç”»ï¼ˆè·ç¨®ã‚’æ¨ªè»¸ã€ã‚«ãƒ†ã‚´ãƒªã‚’ç¸¦è»¸ã«é…ç½®ï¼‰
    fig = px.imshow(
        pivot_df_display,
        labels=dict(x=group_by, y="ã‚«ãƒ†ã‚´ãƒª", color="ä¿æœ‰é‡" if aggregation_method == "mean" else "ä¿æœ‰é‡"),
        aspect="auto",
        color_continuous_scale="Greens",  # è–„ã„ç·‘â†’æ¿ƒã„ç·‘ã®ã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        text_auto=".2f"
    )

    fig.update_layout(
        height=max(400, len(pivot_df) * 50),
        title=f"ã‚«ãƒ†ã‚´ãƒªåˆ¥ Ã— {group_by}åˆ¥ ã‚¹ã‚­ãƒ«ä¿æœ‰çŠ¶æ³ï¼ˆ{aggregation_method == 'mean' and 'å¹³å‡' or 'ä¸­å¤®å€¤'}ï¼‰",
        font=dict(size=11),
        xaxis=dict(
            side='top',  # xè»¸ãƒ©ãƒ™ãƒ«ã‚’ä¸Šã«é…ç½®
            tickangle=-45  # ãƒ©ãƒ™ãƒ«ã‚’æ–œã‚ã«è¡¨ç¤º
        ),
        yaxis=dict(
            tickfont=dict(family="Courier New, monospace")  # ç­‰å¹…ãƒ•ã‚©ãƒ³ãƒˆã§ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆã‚’æ­£ã—ãè¡¨ç¤º
        )
    )

    st.plotly_chart(fig, use_container_width=True)

    # äººæ•°è¡¨ç¤ºï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    if show_count:
        st.markdown("#### ğŸ‘¥ ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥äººæ•°")
        count_df = filtered_df.groupby(["ã‚«ãƒ†ã‚´ãƒªéšå±¤", group_by]).size().unstack(fill_value=0)
        st.dataframe(count_df, use_container_width=True)

    # ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    csv = pivot_df.to_csv(index=True).encode('utf-8-sig')
    st.download_button(
        label=f"ğŸ“¥ {group_by}åˆ¥ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (CSV)",
        data=csv,
        file_name=f"category_{group_by}_analysis.csv",
        mime="text/csv"
    )


def render_job_role_skill_heatmap(
    member_competence_df: pd.DataFrame,
    competence_master_df: pd.DataFrame,
    members_df: pd.DataFrame
) -> None:
    """
    â‘¡è·ç¨®Ã—å½¹è·åˆ¥ã‚¹ã‚­ãƒ«é›†è¨ˆãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

    Args:
        member_competence_df: ãƒ¡ãƒ³ãƒãƒ¼åŠ›é‡ãƒãƒˆãƒªã‚¯ã‚¹
        competence_master_df: åŠ›é‡ãƒã‚¹ã‚¿
        members_df: ãƒ¡ãƒ³ãƒãƒ¼ãƒã‚¹ã‚¿
    """
    st.markdown("### ğŸ¯ è·ç¨® Ã— å½¹è·åˆ¥ ã‚¹ã‚­ãƒ«é›†è¨ˆ")

    # ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãƒ‘ãƒãƒ«
    col1, col2, col3 = st.columns(3)

    with col1:
        aggregation_method = st.selectbox(
            "é›†è¨ˆæ–¹æ³•",
            options=["mean", "median"],
            format_func=lambda x: "å¹³å‡å€¤" if x == "mean" else "ä¸­å¤®å€¤",
            key="job_role_agg"
        )

    # è·ç¨®ã¨å½¹è·ã®é¸æŠ
    if "è·ç¨®" not in members_df.columns or "å½¹è·" not in members_df.columns:
        st.warning("è·ç¨®ã¾ãŸã¯å½¹è·æƒ…å ±ãŒãƒ¡ãƒ³ãƒãƒ¼ãƒã‚¹ã‚¿ã«å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
        return

    with col2:
        available_jobs = sorted(members_df["è·ç¨®"].dropna().unique())
        selected_jobs = st.multiselect(
            "è¡¨ç¤ºã™ã‚‹è·ç¨®",
            options=available_jobs,
            default=available_jobs,
            key="job_select"
        )

    with col3:
        available_roles = sorted(members_df["å½¹è·"].dropna().unique())
        selected_roles = st.multiselect(
            "è¡¨ç¤ºã™ã‚‹å½¹è·",
            options=available_roles,
            default=available_roles,
            key="role_select"
        )

    if not selected_jobs or not selected_roles:
        st.info("è·ç¨®ã¨å½¹è·ã‚’é¸æŠã—ã¦ãã ã•ã„")
        return

    # ãƒ‡ãƒ¼ã‚¿æº–å‚™
    filtered_members = members_df[
        (members_df["è·ç¨®"].isin(selected_jobs)) &
        (members_df["å½¹è·"].isin(selected_roles))
    ]

    # ãƒ¡ãƒ³ãƒãƒ¼åŠ›é‡ã«ãƒ¡ãƒ³ãƒãƒ¼æƒ…å ±ã‚’çµåˆ
    merged_df = member_competence_df.merge(
        filtered_members[["ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰", "è·ç¨®", "å½¹è·"]],
        on="ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰",
        how="inner"
    )

    # ä¿æœ‰é‡ã‚«ãƒ©ãƒ ã®æ¤œå‡º
    level_col = None
    for col in ["ä¿æœ‰é‡", "åŠ›é‡ãƒ¬ãƒ™ãƒ«", "ãƒ¬ãƒ™ãƒ«"]:
        if col in merged_df.columns:
            level_col = col
            break

    if level_col is None:
        st.warning("ä¿æœ‰é‡ã¾ãŸã¯ãƒ¬ãƒ™ãƒ«æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return

    # ä¿æœ‰é‡ã‚’æ•°å€¤å‹ã«å¤‰æ›ï¼ˆã‚¨ãƒ©ãƒ¼å›é¿ï¼‰
    merged_df[level_col] = pd.to_numeric(merged_df[level_col], errors='coerce')

    # NaNã‚’é™¤å¤–
    merged_df = merged_df.dropna(subset=[level_col])

    if len(merged_df) == 0:
        st.warning("æœ‰åŠ¹ãªä¿æœ‰é‡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return

    # è·ç¨®Ã—å½¹è·ã§ã‚¯ãƒ­ã‚¹é›†è¨ˆ
    if aggregation_method == "mean":
        pivot_df = merged_df.groupby(["è·ç¨®", "å½¹è·"])[level_col].mean().unstack(fill_value=0)
    else:
        pivot_df = merged_df.groupby(["è·ç¨®", "å½¹è·"])[level_col].median().unstack(fill_value=0)

    # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—æç”»ï¼ˆå½¹è·ã‚’æ¨ªè»¸ã€è·ç¨®ã‚’ç¸¦è»¸ã«é…ç½®ï¼‰
    fig = px.imshow(
        pivot_df,
        labels=dict(x="å½¹è·", y="è·ç¨®", color="ä¿æœ‰é‡"),
        aspect="auto",
        color_continuous_scale="Greens",  # è–„ã„ç·‘â†’æ¿ƒã„ç·‘ã®ã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        text_auto=".2f"
    )

    fig.update_layout(
        height=max(400, len(pivot_df) * 60),
        title=f"è·ç¨® Ã— å½¹è·åˆ¥ å¹³å‡ã‚¹ã‚­ãƒ«ä¿æœ‰çŠ¶æ³ï¼ˆ{aggregation_method == 'mean' and 'å¹³å‡' or 'ä¸­å¤®å€¤'}ï¼‰",
        font=dict(size=12),
        xaxis=dict(
            side='top',  # xè»¸ãƒ©ãƒ™ãƒ«ã‚’ä¸Šã«é…ç½®
            tickangle=-45  # ãƒ©ãƒ™ãƒ«ã‚’æ–œã‚ã«è¡¨ç¤º
        )
    )

    st.plotly_chart(fig, use_container_width=True)

    # çµ±è¨ˆã‚µãƒãƒªãƒ¼
    st.markdown("#### ğŸ“ˆ çµ±è¨ˆã‚µãƒãƒªãƒ¼")
    col1, col2, col3 = st.columns(3)

    with col1:
        st.metric("æœ€é«˜ã‚¹ã‚­ãƒ«ä¿æœ‰", f"{pivot_df.max().max():.2f}")
    with col2:
        st.metric("æœ€ä½ã‚¹ã‚­ãƒ«ä¿æœ‰", f"{pivot_df.min().min():.2f}")
    with col3:
        st.metric("å…¨ä½“å¹³å‡", f"{pivot_df.mean().mean():.2f}")

    # äººæ•°ãƒãƒˆãƒªã‚¯ã‚¹
    st.markdown("#### ğŸ‘¥ è·ç¨® Ã— å½¹è·åˆ¥ äººæ•°")
    count_df = merged_df.groupby(["è·ç¨®", "å½¹è·"]).size().unstack(fill_value=0)
    st.dataframe(count_df, use_container_width=True)


def render_skill_portfolio_analysis(
    member_competence_df: pd.DataFrame,
    competence_master_df: pd.DataFrame,
    members_df: pd.DataFrame
) -> None:
    """
    â‘¢ã‚¹ã‚­ãƒ«ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªåˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

    çµ„ç¹”ã®ã‚¹ã‚­ãƒ«å¤šæ§˜æ€§ã€é›†ä¸­åº¦ã€ãƒãƒ©ãƒ³ã‚¹ã‚’åˆ†æ
    """
    st.markdown("### ğŸ’¼ ã‚¹ã‚­ãƒ«ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªåˆ†æ")
    st.markdown("""
    **ç›®çš„**: çµ„ç¹”ã®ã‚¹ã‚­ãƒ«ä¿æœ‰ã®ãƒãƒ©ãƒ³ã‚¹ã‚’è©•ä¾¡ã—ã€ãƒªã‚¹ã‚¯ã‚’ç‰¹å®šã—ã¾ã™
    - ğŸ”´ **é«˜ãƒªã‚¹ã‚¯**: ç‰¹å®šã‚¹ã‚­ãƒ«ã«ä¾å­˜ï¼ˆä¿æœ‰è€…ãŒå°‘ãªã„ï¼‰
    - ğŸŸ¡ **ä¸­ãƒªã‚¹ã‚¯**: ã‚¹ã‚­ãƒ«ååœ¨ã‚ã‚Š
    - ğŸŸ¢ **ä½ãƒªã‚¹ã‚¯**: ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ª
    """)

    # ã‚¹ã‚­ãƒ«ä¿æœ‰è€…æ•°ã®åˆ†å¸ƒåˆ†æ
    skill_holders = member_competence_df.groupby("åŠ›é‡ã‚³ãƒ¼ãƒ‰").size().reset_index(name="ä¿æœ‰è€…æ•°")

    # åŠ›é‡ãƒã‚¹ã‚¿ã‹ã‚‰å¿…è¦ãªã‚«ãƒ©ãƒ ã‚’å–å¾—ï¼ˆå­˜åœ¨ãƒã‚§ãƒƒã‚¯ï¼‰
    master_cols = ["åŠ›é‡ã‚³ãƒ¼ãƒ‰", "åŠ›é‡å"]
    if "åŠ›é‡ã‚«ãƒ†ã‚´ãƒªãƒ¼å" in competence_master_df.columns:
        master_cols.append("åŠ›é‡ã‚«ãƒ†ã‚´ãƒªãƒ¼å")

    skill_holders = skill_holders.merge(
        competence_master_df[master_cols],
        on="åŠ›é‡ã‚³ãƒ¼ãƒ‰",
        how="left"
    )

    # ãƒªã‚¹ã‚¯åˆ†é¡
    total_members = len(members_df)
    skill_holders["ä¿æœ‰ç‡"] = skill_holders["ä¿æœ‰è€…æ•°"] / total_members

    def classify_risk(holder_count):
        if holder_count == 1:
            return "ğŸ”´ é«˜ãƒªã‚¹ã‚¯ï¼ˆ1åã®ã¿ï¼‰"
        elif holder_count <= 3:
            return "ğŸŸ  ä¸­é«˜ãƒªã‚¹ã‚¯ï¼ˆ2-3åï¼‰"
        elif holder_count <= 5:
            return "ğŸŸ¡ ä¸­ãƒªã‚¹ã‚¯ï¼ˆ4-5åï¼‰"
        else:
            return "ğŸŸ¢ ä½ãƒªã‚¹ã‚¯ï¼ˆ6åä»¥ä¸Šï¼‰"

    skill_holders["ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«"] = skill_holders["ä¿æœ‰è€…æ•°"].apply(classify_risk)

    # ãƒªã‚¹ã‚¯åˆ†å¸ƒ
    col1, col2 = st.columns(2)

    with col1:
        st.markdown("#### ğŸ¯ ã‚¹ã‚­ãƒ«ãƒªã‚¹ã‚¯åˆ†å¸ƒ")
        risk_dist = skill_holders["ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«"].value_counts().reset_index()
        risk_dist.columns = ["ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«", "ã‚¹ã‚­ãƒ«æ•°"]

        fig = px.pie(
            risk_dist,
            values="ã‚¹ã‚­ãƒ«æ•°",
            names="ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«",
            title="ã‚¹ã‚­ãƒ«ä¿æœ‰ãƒªã‚¹ã‚¯åˆ†å¸ƒ",
            color_discrete_sequence=["#d62728", "#ff7f0e", "#ffbb78", "#2ca02c"]  # èµ¤â†’ã‚ªãƒ¬ãƒ³ã‚¸â†’é»„â†’ç·‘
        )
        st.plotly_chart(fig, use_container_width=True)

    with col2:
        st.markdown("#### ğŸ“Š ä¿æœ‰è€…æ•°åˆ†å¸ƒ")
        fig = px.histogram(
            skill_holders,
            x="ä¿æœ‰è€…æ•°",
            nbins=20,
            title="ã‚¹ã‚­ãƒ«ä¿æœ‰è€…æ•°ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ",
            labels={"ä¿æœ‰è€…æ•°": "ä¿æœ‰è€…æ•°", "count": "ã‚¹ã‚­ãƒ«æ•°"}
        )
        st.plotly_chart(fig, use_container_width=True)

    # é«˜ãƒªã‚¹ã‚¯ã‚¹ã‚­ãƒ«ä¸€è¦§
    st.markdown("#### âš ï¸ é«˜ãƒªã‚¹ã‚¯ã‚¹ã‚­ãƒ«ï¼ˆä¿æœ‰è€…3åä»¥ä¸‹ï¼‰")
    high_risk_skills = skill_holders[skill_holders["ä¿æœ‰è€…æ•°"] <= 3].sort_values("ä¿æœ‰è€…æ•°")

    if len(high_risk_skills) > 0:
        # è¡¨ç¤ºã™ã‚‹ã‚«ãƒ©ãƒ ã‚’å‹•çš„ã«æ±ºå®š
        display_cols = ["åŠ›é‡å"]
        if "åŠ›é‡ã‚«ãƒ†ã‚´ãƒªãƒ¼å" in high_risk_skills.columns:
            display_cols.append("åŠ›é‡ã‚«ãƒ†ã‚´ãƒªãƒ¼å")
        display_cols.extend(["ä¿æœ‰è€…æ•°", "ä¿æœ‰ç‡", "ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«"])

        st.dataframe(
            high_risk_skills[display_cols],
            use_container_width=True,
            height=300
        )

        st.warning(f"âš ï¸ {len(high_risk_skills)}ä»¶ã®ã‚¹ã‚­ãƒ«ãŒé«˜ãƒªã‚¹ã‚¯çŠ¶æ…‹ã§ã™ã€‚å„ªå…ˆçš„ã«è‚²æˆè¨ˆç”»ã‚’ç«‹æ¡ˆã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
    else:
        st.success("âœ… é«˜ãƒªã‚¹ã‚¯ã‚¹ã‚­ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“")

    # ã‚¹ã‚­ãƒ«ã‚«ãƒ†ã‚´ãƒªåˆ¥é›†ä¸­åº¦åˆ†æ
    if "åŠ›é‡ã‚«ãƒ†ã‚´ãƒªãƒ¼å" in skill_holders.columns:
        st.markdown("#### ğŸ“‚ ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚¹ã‚­ãƒ«é›†ä¸­åº¦")

        category_summary = skill_holders.groupby("åŠ›é‡ã‚«ãƒ†ã‚´ãƒªãƒ¼å").agg({
            "ä¿æœ‰è€…æ•°": ["mean", "min", "max", "std"]
        }).reset_index()
        category_summary.columns = ["ã‚«ãƒ†ã‚´ãƒª", "å¹³å‡ä¿æœ‰è€…æ•°", "æœ€å°ä¿æœ‰è€…æ•°", "æœ€å¤§ä¿æœ‰è€…æ•°", "æ¨™æº–åå·®"]
        category_summary["å¤‰å‹•ä¿‚æ•° (CV)"] = category_summary["æ¨™æº–åå·®"] / category_summary["å¹³å‡ä¿æœ‰è€…æ•°"]
        category_summary = category_summary.sort_values("å¤‰å‹•ä¿‚æ•° (CV)", ascending=False)

        st.dataframe(category_summary, use_container_width=True)
        st.caption("ğŸ’¡ å¤‰å‹•ä¿‚æ•°(CV)ãŒé«˜ã„ã‚«ãƒ†ã‚´ãƒªã¯ã€ã‚¹ã‚­ãƒ«é–“ã®ä¿æœ‰è€…æ•°ã®ã°ã‚‰ã¤ããŒå¤§ããã€ãƒªã‚¹ã‚¯ãŒé«˜ã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")


def render_talent_risk_dashboard(
    member_competence_df: pd.DataFrame,
    competence_master_df: pd.DataFrame,
    members_df: pd.DataFrame
) -> None:
    """
    â‘£äººæãƒªã‚¹ã‚¯åˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

    ã‚­ãƒ¼ãƒ‘ãƒ¼ã‚½ãƒ³ãƒªã‚¹ã‚¯ã€ã‚¹ã‚­ãƒ«ä¾å­˜åº¦ã‚’åˆ†æ
    """
    st.markdown("### ğŸš¨ äººæãƒªã‚¹ã‚¯åˆ†æ")
    st.markdown("""
    **åˆ†æç›®çš„**: ç‰¹å®šãƒ¡ãƒ³ãƒãƒ¼ã¸ã®ã‚¹ã‚­ãƒ«é›†ä¸­ãƒªã‚¹ã‚¯ã‚’ç‰¹å®šã—ã€çµ„ç¹”ã®è„†å¼±æ€§ã‚’å¯è¦–åŒ–
    """)

    # ãƒ¡ãƒ³ãƒãƒ¼åˆ¥ã‚¹ã‚­ãƒ«ä¿æœ‰æ•°
    member_skill_counts = member_competence_df.groupby("ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰").size().reset_index(name="ä¿æœ‰ã‚¹ã‚­ãƒ«æ•°")

    # ãƒ¡ãƒ³ãƒãƒ¼æƒ…å ±ã®çµåˆï¼ˆã‚«ãƒ©ãƒ å­˜åœ¨ãƒã‚§ãƒƒã‚¯ï¼‰
    member_cols = ["ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰"]
    optional_cols = {"ãƒ¡ãƒ³ãƒãƒ¼å": "ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰", "è·ç¨®": None, "å½¹è·": None}  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å€¤

    for col, fallback in optional_cols.items():
        if col in members_df.columns:
            member_cols.append(col)
        elif fallback:
            # ãƒ¡ãƒ³ãƒãƒ¼åãŒãªã„å ´åˆã¯ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰ã§ä»£ç”¨
            if col == "ãƒ¡ãƒ³ãƒãƒ¼å":
                members_df = members_df.copy()
                members_df["ãƒ¡ãƒ³ãƒãƒ¼å"] = members_df["ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰"]
                member_cols.append("ãƒ¡ãƒ³ãƒãƒ¼å")

    member_skill_counts = member_skill_counts.merge(
        members_df[member_cols],
        on="ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰",
        how="left"
    )

    # ãƒ¡ãƒ³ãƒãƒ¼åã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ãªã„å ´åˆã®å¯¾å¿œ
    if "ãƒ¡ãƒ³ãƒãƒ¼å" not in member_skill_counts.columns:
        member_skill_counts["ãƒ¡ãƒ³ãƒãƒ¼å"] = member_skill_counts["ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰"]

    # ä¸Šä½ã‚¹ã‚­ãƒ«ä¿æœ‰è€…
    st.markdown("#### ğŸŒŸ ãƒˆãƒƒãƒ—ã‚¹ã‚­ãƒ«ä¿æœ‰è€…ï¼ˆçµ„ç¹”ã®ã‚­ãƒ¼ãƒ‘ãƒ¼ã‚½ãƒ³ï¼‰")
    top_members = member_skill_counts.nlargest(10, "ä¿æœ‰ã‚¹ã‚­ãƒ«æ•°")

    col1, col2 = st.columns([2, 1])

    with col1:
        fig = px.bar(
            top_members,
            x="ãƒ¡ãƒ³ãƒãƒ¼å",
            y="ä¿æœ‰ã‚¹ã‚­ãƒ«æ•°",
            color="è·ç¨®",
            title="ãƒˆãƒƒãƒ—10ã‚¹ã‚­ãƒ«ä¿æœ‰è€…",
            text="ä¿æœ‰ã‚¹ã‚­ãƒ«æ•°"
        )
        fig.update_traces(texttemplate='%{text}', textposition='outside')
        fig.update_layout(height=400)
        st.plotly_chart(fig, use_container_width=True)

    with col2:
        st.metric("å¹³å‡ã‚¹ã‚­ãƒ«æ•°", f"{member_skill_counts['ä¿æœ‰ã‚¹ã‚­ãƒ«æ•°'].mean():.1f}")
        st.metric("ä¸­å¤®å€¤", f"{member_skill_counts['ä¿æœ‰ã‚¹ã‚­ãƒ«æ•°'].median():.0f}")
        st.metric("æœ€å¤§å€¤", f"{member_skill_counts['ä¿æœ‰ã‚¹ã‚­ãƒ«æ•°'].max()}")

        # ãƒ‘ãƒ¬ãƒ¼ãƒˆåˆ†æï¼ˆä¸Šä½20%ãŒä½•%ã®ã‚¹ã‚­ãƒ«ã‚’ä¿æœ‰ã—ã¦ã„ã‚‹ã‹ï¼‰
        top_20_pct_count = int(len(member_skill_counts) * 0.2)
        top_20_pct_skills = member_skill_counts.nlargest(top_20_pct_count, "ä¿æœ‰ã‚¹ã‚­ãƒ«æ•°")["ä¿æœ‰ã‚¹ã‚­ãƒ«æ•°"].sum()
        total_skills = member_skill_counts["ä¿æœ‰ã‚¹ã‚­ãƒ«æ•°"].sum()
        pareto_ratio = (top_20_pct_skills / total_skills) * 100

        st.metric(
            "ãƒ‘ãƒ¬ãƒ¼ãƒˆæ¯”ç‡",
            f"{pareto_ratio:.1f}%",
            help="ä¸Šä½20%ã®ãƒ¡ãƒ³ãƒãƒ¼ãŒä¿æœ‰ã™ã‚‹ã‚¹ã‚­ãƒ«ã®å‰²åˆ"
        )

    # ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚¹ã‚­ãƒ«åˆ†æï¼ˆãã®ãƒ¡ãƒ³ãƒãƒ¼ã—ã‹æŒã£ã¦ã„ãªã„ã‚¹ã‚­ãƒ«ï¼‰
    st.markdown("#### ğŸ¯ ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚¹ã‚­ãƒ«ä¿æœ‰è€…ï¼ˆé›¢è·ãƒªã‚¹ã‚¯é«˜ï¼‰")

    skill_holder_counts = member_competence_df.groupby("åŠ›é‡ã‚³ãƒ¼ãƒ‰")["ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰"].nunique().reset_index(name="ä¿æœ‰è€…æ•°")
    unique_skills = skill_holder_counts[skill_holder_counts["ä¿æœ‰è€…æ•°"] == 1]["åŠ›é‡ã‚³ãƒ¼ãƒ‰"].tolist()

    if unique_skills:
        unique_skill_holders = member_competence_df[
            member_competence_df["åŠ›é‡ã‚³ãƒ¼ãƒ‰"].isin(unique_skills)
        ].merge(
            members_df[member_cols],  # æ—¢ã«æ§‹ç¯‰ã—ãŸmember_colsã‚’ä½¿ç”¨
            on="ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰",
            how="left"
        ).merge(
            competence_master_df[["åŠ›é‡ã‚³ãƒ¼ãƒ‰", "åŠ›é‡å"]],
            on="åŠ›é‡ã‚³ãƒ¼ãƒ‰",
            how="left"
        )

        # ãƒ¡ãƒ³ãƒãƒ¼åã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ãªã„å ´åˆã®å¯¾å¿œ
        if "ãƒ¡ãƒ³ãƒãƒ¼å" not in unique_skill_holders.columns:
            unique_skill_holders["ãƒ¡ãƒ³ãƒãƒ¼å"] = unique_skill_holders["ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰"]

        # groupbyã®ã‚«ãƒ©ãƒ ã‚’å‹•çš„ã«æ§‹ç¯‰
        groupby_cols = ["ãƒ¡ãƒ³ãƒãƒ¼å"]
        if "è·ç¨®" in unique_skill_holders.columns:
            groupby_cols.append("è·ç¨®")
        if "å½¹è·" in unique_skill_holders.columns:
            groupby_cols.append("å½¹è·")

        unique_summary = unique_skill_holders.groupby(groupby_cols).agg({
            "åŠ›é‡ã‚³ãƒ¼ãƒ‰": "count"
        }).reset_index()

        # ã‚«ãƒ©ãƒ åã‚’è¨­å®š
        new_cols = groupby_cols + ["ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚¹ã‚­ãƒ«æ•°"]
        unique_summary.columns = new_cols
        unique_summary = unique_summary.sort_values("ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚¹ã‚­ãƒ«æ•°", ascending=False)

        st.dataframe(unique_summary, use_container_width=True, height=300)

        st.error(f"âš ï¸ {len(unique_summary)}åã®ãƒ¡ãƒ³ãƒãƒ¼ãŒçµ„ç¹”ã§å”¯ä¸€ã®ã‚¹ã‚­ãƒ«ã‚’ä¿æœ‰ã—ã¦ã„ã¾ã™ã€‚ã“ã‚Œã‚‰ã®ãƒ¡ãƒ³ãƒãƒ¼ã®é›¢è·ã¯çµ„ç¹”ã«é‡å¤§ãªå½±éŸ¿ã‚’ä¸ãˆã¾ã™ã€‚")

        # è©³ç´°è¡¨ç¤º
        with st.expander("ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚¹ã‚­ãƒ«è©³ç´°ã‚’è¡¨ç¤º"):
            # è¡¨ç¤ºã‚«ãƒ©ãƒ ã‚’å‹•çš„ã«æ§‹ç¯‰
            detail_cols = ["ãƒ¡ãƒ³ãƒãƒ¼å"]
            if "è·ç¨®" in unique_skill_holders.columns:
                detail_cols.append("è·ç¨®")
            if "åŠ›é‡å" in unique_skill_holders.columns:
                detail_cols.append("åŠ›é‡å")

            st.dataframe(
                unique_skill_holders[detail_cols],
                use_container_width=True
            )
    else:
        st.success("âœ… å…¨ã¦ã®ã‚¹ã‚­ãƒ«ãŒè¤‡æ•°åã§å…±æœ‰ã•ã‚Œã¦ã„ã¾ã™")

    # ã‚¹ã‚­ãƒ«åˆ†å¸ƒã®åã‚Šåˆ†æ
    st.markdown("#### ğŸ“Š ã‚¹ã‚­ãƒ«åˆ†å¸ƒã®ä¸å‡è¡¡åº¦")

    # ã‚¸ãƒ‹ä¿‚æ•°ã®è¨ˆç®—ï¼ˆã‚¹ã‚­ãƒ«ä¿æœ‰ã®ä¸å¹³ç­‰åº¦ï¼‰
    skill_counts_sorted = member_skill_counts["ä¿æœ‰ã‚¹ã‚­ãƒ«æ•°"].sort_values().values
    n = len(skill_counts_sorted)
    index = np.arange(1, n + 1)
    gini = (2 * np.sum(index * skill_counts_sorted)) / (n * np.sum(skill_counts_sorted)) - (n + 1) / n

    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("ã‚¸ãƒ‹ä¿‚æ•°", f"{gini:.3f}", help="0ã«è¿‘ã„ã»ã©å‡ç­‰ã€1ã«è¿‘ã„ã»ã©ä¸å‡ç­‰")
    with col2:
        skewness = stats.skew(member_skill_counts["ä¿æœ‰ã‚¹ã‚­ãƒ«æ•°"])
        st.metric("æ­ªåº¦", f"{skewness:.2f}", help="æ­£ã®å€¤ã¯ä¸€éƒ¨ã®ãƒ¡ãƒ³ãƒãƒ¼ã«ã‚¹ã‚­ãƒ«ãŒé›†ä¸­")
    with col3:
        cv = member_skill_counts["ä¿æœ‰ã‚¹ã‚­ãƒ«æ•°"].std() / member_skill_counts["ä¿æœ‰ã‚¹ã‚­ãƒ«æ•°"].mean()
        st.metric("å¤‰å‹•ä¿‚æ•°", f"{cv:.2f}", help="ã‚¹ã‚­ãƒ«ä¿æœ‰æ•°ã®ã°ã‚‰ã¤ãåº¦")

    if gini > 0.4:
        st.warning("âš ï¸ ã‚¹ã‚­ãƒ«ãŒä¸€éƒ¨ã®ãƒ¡ãƒ³ãƒãƒ¼ã«é›†ä¸­ã—ã¦ã„ã¾ã™ã€‚çµ„ç¹”å…¨ä½“ã§ã®ã‚¹ã‚­ãƒ«å…±æœ‰ãƒ»è‚²æˆã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
    elif gini < 0.2:
        st.success("âœ… ã‚¹ã‚­ãƒ«ãŒå‡ç­‰ã«åˆ†æ•£ã—ã¦ã„ã¾ã™ã€‚")
    else:
        st.info("â„¹ï¸ ã‚¹ã‚­ãƒ«åˆ†å¸ƒã¯æ¨™æº–çš„ã§ã™ã€‚")


def render_benchmark_dashboard(
    member_competence_df: pd.DataFrame,
    competence_master_df: pd.DataFrame,
    members_df: pd.DataFrame
) -> None:
    """
    â‘¤çµ„ç¹”ãƒ™ãƒ³ãƒãƒãƒ¼ã‚­ãƒ³ã‚°ï¼†ç«¶åˆæ¯”è¼ƒãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

    æ¥­ç•Œæ¨™æº–ã‚„ç†æƒ³çŠ¶æ…‹ã¨æ¯”è¼ƒ
    """
    st.markdown("### ğŸ“Š çµ„ç¹”ãƒ™ãƒ³ãƒãƒãƒ¼ã‚­ãƒ³ã‚°")
    st.markdown("""
    **åˆ†æç›®çš„**: çµ„ç¹”ã®ã‚¹ã‚­ãƒ«æˆç†Ÿåº¦ã‚’è©•ä¾¡ã—ã€æ”¹å–„é ˜åŸŸã‚’ç‰¹å®š
    """)

    # åŸºæœ¬çµ±è¨ˆ
    total_members = len(members_df)
    total_skills_available = len(competence_master_df)
    total_skill_acquisitions = len(member_competence_df)
    avg_skills_per_member = total_skill_acquisitions / total_members if total_members > 0 else 0
    coverage_rate = (member_competence_df["åŠ›é‡ã‚³ãƒ¼ãƒ‰"].nunique() / total_skills_available) * 100

    # å„æŒ‡æ¨™ã‚’å®‰å…¨ã«è¨ˆç®—
    try:
        diversity_index = calculate_diversity_index(member_competence_df)
    except Exception as e:
        diversity_index = 0.0
        st.warning(f"ã‚¹ã‚­ãƒ«å¤šæ§˜æ€§æŒ‡æ•°ã®è¨ˆç®—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

    try:
        t_shaped_ratio = calculate_t_shaped_ratio(member_competence_df, competence_master_df)
    except Exception as e:
        t_shaped_ratio = 0.0
        st.warning(f"Tå­—å‹äººææ¯”ç‡ã®è¨ˆç®—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¥­ç•Œæ¨™æº–å€¤ - ä»®æƒ³ãƒ‡ãƒ¼ã‚¿ï¼‰
    # å®Ÿéš›ã®ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯å¤–éƒ¨APIã‚„è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—
    benchmark_data = {
        "ç¾åœ¨ã®çµ„ç¹”": {
            "å¹³å‡ã‚¹ã‚­ãƒ«æ•°/äºº": avg_skills_per_member,
            "ã‚¹ã‚­ãƒ«ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡": coverage_rate,
            "ã‚¹ã‚­ãƒ«å¤šæ§˜æ€§æŒ‡æ•°": diversity_index,
            "Tå­—å‹äººææ¯”ç‡": t_shaped_ratio
        },
        "æ¥­ç•Œå¹³å‡": {
            "å¹³å‡ã‚¹ã‚­ãƒ«æ•°/äºº": 8.5,
            "ã‚¹ã‚­ãƒ«ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡": 65.0,
            "ã‚¹ã‚­ãƒ«å¤šæ§˜æ€§æŒ‡æ•°": 0.75,
            "Tå­—å‹äººææ¯”ç‡": 35.0
        },
        "ãƒˆãƒƒãƒ—ä¼æ¥­": {
            "å¹³å‡ã‚¹ã‚­ãƒ«æ•°/äºº": 12.0,
            "ã‚¹ã‚­ãƒ«ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡": 85.0,
            "ã‚¹ã‚­ãƒ«å¤šæ§˜æ€§æŒ‡æ•°": 0.85,
            "Tå­—å‹äººææ¯”ç‡": 50.0
        }
    }

    df_benchmark = pd.DataFrame(benchmark_data).T

    # ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ
    st.markdown("#### ğŸ¯ ç·åˆã‚¹ã‚³ã‚¢æ¯”è¼ƒ")

    categories = list(df_benchmark.columns)

    fig = go.Figure()

    for org_name in df_benchmark.index:
        fig.add_trace(go.Scatterpolar(
            r=df_benchmark.loc[org_name].values,
            theta=categories,
            fill='toself',
            name=org_name
        ))

    fig.update_layout(
        polar=dict(radialaxis=dict(visible=True, range=[0, 100])),
        showlegend=True,
        height=500,
        title="çµ„ç¹”ã‚¹ã‚­ãƒ«æˆç†Ÿåº¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"
    )

    st.plotly_chart(fig, use_container_width=True)

    # è©³ç´°æ¯”è¼ƒãƒ†ãƒ¼ãƒ–ãƒ«
    st.markdown("#### ğŸ“‹ è©³ç´°æ¯”è¼ƒ")

    comparison_df = df_benchmark.copy()
    comparison_df["vs æ¥­ç•Œå¹³å‡"] = ((comparison_df.loc["ç¾åœ¨ã®çµ„ç¹”"] / comparison_df.loc["æ¥­ç•Œå¹³å‡"] - 1) * 100).round(1)
    comparison_df["vs ãƒˆãƒƒãƒ—ä¼æ¥­"] = ((comparison_df.loc["ç¾åœ¨ã®çµ„ç¹”"] / comparison_df.loc["ãƒˆãƒƒãƒ—ä¼æ¥­"] - 1) * 100).round(1)

    st.dataframe(comparison_df.T, use_container_width=True)

    # æ”¹å–„æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
    st.markdown("#### ğŸ’¡ æ”¹å–„æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³")

    actions = []

    if avg_skills_per_member < 8.5:
        actions.append("ğŸ“š **ã‚¹ã‚­ãƒ«è‚²æˆãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®å¼·åŒ–**: å¹³å‡ã‚¹ã‚­ãƒ«æ•°ãŒæ¥­ç•Œå¹³å‡ã‚’ä¸‹å›ã£ã¦ã„ã¾ã™")

    if coverage_rate < 65:
        actions.append("ğŸ¯ **ã‚¹ã‚­ãƒ«ã‚«ãƒãƒ¬ãƒƒã‚¸ã®æ‹¡å¤§**: çµ„ç¹”ã¨ã—ã¦ä¿æœ‰ã™ã¹ãã‚¹ã‚­ãƒ«ã®ç¯„å›²ã‚’åºƒã’ã¾ã—ã‚‡ã†")

    diversity = calculate_diversity_index(member_competence_df)
    if diversity < 0.75:
        actions.append("ğŸŒˆ **ã‚¹ã‚­ãƒ«å¤šæ§˜æ€§ã®å‘ä¸Š**: ç‰¹å®šã‚¹ã‚­ãƒ«ã¸ã®åã‚Šã‚’æ˜¯æ­£ã—ã¾ã—ã‚‡ã†")

    t_shaped = calculate_t_shaped_ratio(member_competence_df, competence_master_df)
    if t_shaped < 35:
        actions.append("ğŸ”° **Tå­—å‹äººæã®è‚²æˆ**: å°‚é–€æ€§ã¨å¹…åºƒã„çŸ¥è­˜ã‚’æŒã¤äººæã‚’å¢—ã‚„ã—ã¾ã—ã‚‡ã†")

    if actions:
        for action in actions:
            st.markdown(f"- {action}")
    else:
        st.success("âœ… å…¨ã¦ã®æŒ‡æ¨™ã§æ¥­ç•Œå¹³å‡ä»¥ä¸Šã‚’é”æˆã—ã¦ã„ã¾ã™ï¼")


def calculate_diversity_index(member_competence_df: pd.DataFrame) -> float:
    """
    ã‚¹ã‚­ãƒ«å¤šæ§˜æ€§æŒ‡æ•°ã‚’è¨ˆç®—ï¼ˆShannon Entropyï¼‰
    """
    skill_counts = member_competence_df["åŠ›é‡ã‚³ãƒ¼ãƒ‰"].value_counts()
    proportions = skill_counts / skill_counts.sum()
    entropy = -np.sum(proportions * np.log(proportions + 1e-10))
    max_entropy = np.log(len(skill_counts))
    return (entropy / max_entropy) * 100 if max_entropy > 0 else 0


def calculate_t_shaped_ratio(member_competence_df: pd.DataFrame, competence_master_df: pd.DataFrame) -> float:
    """
    Tå­—å‹äººææ¯”ç‡ã‚’è¨ˆç®—

    Tå­—å‹ = 1ã¤ä»¥ä¸Šã®æ·±ã„å°‚é–€æ€§ï¼ˆãƒ¬ãƒ™ãƒ«4ä»¥ä¸Šï¼‰ + å¹…åºƒã„çŸ¥è­˜ï¼ˆ3ã‚«ãƒ†ã‚´ãƒªä»¥ä¸Šï¼‰
    """
    # ä¿æœ‰é‡ã‚«ãƒ©ãƒ ã®æ¤œå‡º
    level_col = None
    for col in ["ä¿æœ‰é‡", "åŠ›é‡ãƒ¬ãƒ™ãƒ«", "ãƒ¬ãƒ™ãƒ«"]:
        if col in member_competence_df.columns:
            level_col = col
            break

    if level_col is None:
        return 0.0

    # åŠ›é‡ã‚«ãƒ†ã‚´ãƒªãƒ¼åã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯è¨ˆç®—ä¸å¯
    if "åŠ›é‡ã‚«ãƒ†ã‚´ãƒªãƒ¼å" not in competence_master_df.columns:
        return 0.0

    # ã‚«ãƒ†ã‚´ãƒªæƒ…å ±ã‚’çµåˆ
    merged = member_competence_df.merge(
        competence_master_df[["åŠ›é‡ã‚³ãƒ¼ãƒ‰", "åŠ›é‡ã‚«ãƒ†ã‚´ãƒªãƒ¼å"]],
        on="åŠ›é‡ã‚³ãƒ¼ãƒ‰",
        how="left"
    )

    # ä¿æœ‰é‡ã‚’æ•°å€¤å‹ã«å¤‰æ›
    merged[level_col] = pd.to_numeric(merged[level_col], errors='coerce')

    t_shaped_count = 0
    total_members = merged["ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰"].nunique()

    for member_code in merged["ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰"].unique():
        member_data = merged[merged["ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰"] == member_code]

        # æ·±ã„å°‚é–€æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ¬ãƒ™ãƒ«4ä»¥ä¸Šã®ã‚¹ã‚­ãƒ«ãŒã‚ã‚‹ã‹ï¼‰
        has_deep_skill = False
        try:
            valid_levels = member_data[level_col].dropna()
            if len(valid_levels) > 0 and (valid_levels >= 4).any():
                has_deep_skill = True
        except:
            pass

        # å¹…åºƒã„çŸ¥è­˜ãƒã‚§ãƒƒã‚¯ï¼ˆ3ã‚«ãƒ†ã‚´ãƒªä»¥ä¸Šï¼‰
        category_count = member_data["åŠ›é‡ã‚«ãƒ†ã‚´ãƒªãƒ¼å"].nunique()
        has_broad_knowledge = category_count >= 3

        if has_deep_skill and has_broad_knowledge:
            t_shaped_count += 1

    return (t_shaped_count / total_members * 100) if total_members > 0 else 0.0
