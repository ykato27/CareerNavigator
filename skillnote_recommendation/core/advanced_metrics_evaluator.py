"""
Advanced Metrics Evaluator

GAFAãƒ¬ãƒ™ãƒ«ã®åŒ…æ‹¬çš„ãªè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ :
- ãƒã‚¤ã‚¢ã‚¹è©•ä¾¡ï¼ˆPopularity bias, Position bias, Filter bubbleï¼‰
- å¤šæ§˜æ€§ãƒ»ã‚«ãƒãƒ¬ãƒƒã‚¸è©•ä¾¡ï¼ˆILD, Serendipity, Personalization, Calibrationï¼‰
- åŒ…æ‹¬çš„è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ

å•é¡Œæ„è­˜:
- ã‚ªãƒ•ãƒ©ã‚¤ãƒ³è©•ä¾¡ã®ã¿ï¼ˆPrecision, Recall, NDCGï¼‰ã§ã¯ãƒ“ã‚¸ãƒã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨ã®ç›¸é–¢ãŒä¸æ˜
- ãƒã‚¤ã‚¢ã‚¹è©•ä¾¡ãŒãªã„ã¨ã€äººæ°—ã‚¹ã‚­ãƒ«éå‰°æ¨è–¦ã‚„ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒãƒ–ãƒ«ã«é™¥ã‚‹
- å¤šæ§˜æ€§ãŒä¸ååˆ†ã ã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼æº€è¶³åº¦ãŒä½ä¸‹

Reference:
- Hu et al. (2008): Collaborative Filtering for Implicit Feedback Datasets
- Vargas & Castells (2011): Rank and Relevance in Novelty and Diversity Metrics
- Abdollahpouri et al. (2019): Managing Popularity Bias in Recommender Systems
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional
from collections import defaultdict, Counter
from dataclasses import dataclass
import warnings

from .evaluator import RecommendationEvaluator


@dataclass
class ComprehensiveEvaluationResult:
    """åŒ…æ‹¬çš„è©•ä¾¡çµæœ

    Attributes:
        accuracy_metrics: ç²¾åº¦è©•ä¾¡ï¼ˆPrecision, Recall, NDCGç­‰ï¼‰
        bias_metrics: ãƒã‚¤ã‚¢ã‚¹è©•ä¾¡ï¼ˆPopularity, Position, Filter bubbleï¼‰
        diversity_metrics: å¤šæ§˜æ€§è©•ä¾¡ï¼ˆILD, Serendipity, Personalizationï¼‰
        coverage_metrics: ã‚«ãƒãƒ¬ãƒƒã‚¸è©•ä¾¡ï¼ˆCoverage, Catalog Coverageï¼‰
        calibration_metrics: ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è©•ä¾¡ï¼ˆKL divergenceï¼‰
        overall_score: ç·åˆã‚¹ã‚³ã‚¢ï¼ˆé‡ã¿ä»˜ãå¹³å‡ï¼‰
    """
    accuracy_metrics: Dict[str, float]
    bias_metrics: Dict[str, float]
    diversity_metrics: Dict[str, float]
    coverage_metrics: Dict[str, float]
    calibration_metrics: Dict[str, float]
    overall_score: float


class AdvancedMetricsEvaluator(RecommendationEvaluator):
    """é«˜åº¦ãªè©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æä¾›ã™ã‚‹ã‚¯ãƒ©ã‚¹

    RecommendationEvaluatorã‚’æ‹¡å¼µã—ã€ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’è¿½åŠ :
    1. ãƒã‚¤ã‚¢ã‚¹è©•ä¾¡ï¼ˆPopularity, Position, Filter bubbleï¼‰
    2. å¤šæ§˜æ€§è©•ä¾¡ï¼ˆILD, Serendipity, Personalizationï¼‰
    3. ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è©•ä¾¡ï¼ˆKL divergenceï¼‰
    4. åŒ…æ‹¬çš„è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ

    Usage:
        >>> evaluator = AdvancedMetricsEvaluator()
        >>> result = evaluator.comprehensive_evaluation(
        ...     train_data=train_df,
        ...     test_data=test_df,
        ...     competence_master=master_df,
        ...     top_k=10
        ... )
        >>> print(result.overall_score)
        >>> evaluator.print_comprehensive_report(result)
    """

    def evaluate_popularity_bias(
        self,
        recommendations_list: List[List],
        member_competence: pd.DataFrame,
    ) -> Dict[str, float]:
        """
        Popularity Biasï¼ˆäººæ°—ã‚¹ã‚­ãƒ«éå‰°æ¨è–¦ï¼‰ã‚’è©•ä¾¡

        äººæ°—ã‚¹ã‚­ãƒ«ã°ã‹ã‚Šæ¨è–¦ã—ã¦ã„ãªã„ã‹ã‚’è©•ä¾¡ã€‚
        å¥å…¨ãªæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã¯ãƒ­ãƒ³ã‚°ãƒ†ãƒ¼ãƒ«ã‚¹ã‚­ãƒ«ã‚‚æ¨è–¦ã™ã¹ãã€‚

        Args:
            recommendations_list: ãƒ¡ãƒ³ãƒãƒ¼ã”ã¨ã®æ¨è–¦çµæœãƒªã‚¹ãƒˆ
            member_competence: ãƒ¡ãƒ³ãƒãƒ¼ç¿’å¾—åŠ›é‡ãƒ‡ãƒ¼ã‚¿ï¼ˆäººæ°—åº¦è¨ˆç®—ç”¨ï¼‰

        Returns:
            {
                'popularity_bias': float,  # æ¨è–¦ã‚¹ã‚­ãƒ«ã®å¹³å‡äººæ°—åº¦ï¼ˆ0-1ã€é«˜ã„ã»ã©ãƒã‚¤ã‚¢ã‚¹å¤§ï¼‰
                'tail_ratio': float,  # ãƒ­ãƒ³ã‚°ãƒ†ãƒ¼ãƒ«æ¨è–¦ç‡ï¼ˆ0-1ã€é«˜ã„ã»ã©å¥å…¨ï¼‰
                'gini_coefficient': float,  # æ¨è–¦åˆ†å¸ƒã®Giniä¿‚æ•°ï¼ˆ0-1ã€é«˜ã„ã»ã©åã‚Šå¤§ï¼‰
            }

        Reference:
            Abdollahpouri et al. (2019): Managing Popularity Bias in Recommender Systems
        """
        if not recommendations_list or len(recommendations_list) == 0:
            return {
                'popularity_bias': 0.0,
                'tail_ratio': 0.0,
                'gini_coefficient': 0.0,
            }

        # å„ã‚¹ã‚­ãƒ«ã®äººæ°—åº¦ã‚’è¨ˆç®—ï¼ˆä¿æœ‰è€…æ•°ï¼‰
        competence_counts = member_competence['åŠ›é‡ã‚³ãƒ¼ãƒ‰'].value_counts()
        total_members = member_competence['ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰'].nunique()

        # äººæ°—åº¦ã‚’æ­£è¦åŒ–ï¼ˆ0-1ï¼‰
        competence_popularity = {
            code: count / total_members
            for code, count in competence_counts.items()
        }

        # ãƒ­ãƒ³ã‚°ãƒ†ãƒ¼ãƒ«å®šç¾©ï¼ˆäººæ°—åº¦ãŒä¸­å¤®å€¤ä»¥ä¸‹ï¼‰
        popularity_values = list(competence_popularity.values())
        median_popularity = np.median(popularity_values) if popularity_values else 0.0

        # æ¨è–¦ã•ã‚ŒãŸå„ã‚¹ã‚­ãƒ«ã®äººæ°—åº¦ã‚’é›†è¨ˆ
        recommended_popularities = []
        tail_count = 0
        total_recommendations = 0

        for recommendations in recommendations_list:
            for rec in recommendations:
                popularity = competence_popularity.get(rec.competence_code, 0.0)
                recommended_popularities.append(popularity)
                total_recommendations += 1

                # ãƒ­ãƒ³ã‚°ãƒ†ãƒ¼ãƒ«ã‚¹ã‚­ãƒ«ã‹åˆ¤å®š
                if popularity <= median_popularity:
                    tail_count += 1

        # Popularity Bias: æ¨è–¦ã‚¹ã‚­ãƒ«ã®å¹³å‡äººæ°—åº¦ï¼ˆé«˜ã„ã»ã©ãƒã‚¤ã‚¢ã‚¹å¤§ï¼‰
        avg_popularity = np.mean(recommended_popularities) if recommended_popularities else 0.0

        # Tail Ratio: ãƒ­ãƒ³ã‚°ãƒ†ãƒ¼ãƒ«æ¨è–¦ç‡ï¼ˆé«˜ã„ã»ã©å¥å…¨ï¼‰
        tail_ratio = tail_count / total_recommendations if total_recommendations > 0 else 0.0

        # Gini Coefficient: æ¨è–¦åˆ†å¸ƒã®åã‚Š
        # æ—¢å­˜ã®Giniè¨ˆç®—ã‚’æµç”¨
        recommendation_counts = Counter([rec.competence_code for recs in recommendations_list for rec in recs])
        gini = self._calculate_gini_index(recommendation_counts)

        return {
            'popularity_bias': avg_popularity,
            'tail_ratio': tail_ratio,
            'gini_coefficient': gini,
        }

    def evaluate_position_bias(
        self,
        recommendations_list: List[List],
        actual_acquired_list: List[List[str]],
    ) -> Dict[str, float]:
        """
        Position Biasï¼ˆãƒ©ãƒ³ã‚­ãƒ³ã‚°ä½ç½®ãƒã‚¤ã‚¢ã‚¹ï¼‰ã‚’è©•ä¾¡

        ãƒ©ãƒ³ã‚­ãƒ³ã‚°ä¸Šä½ã»ã©ã‚¯ãƒªãƒƒã‚¯ã•ã‚Œã‚‹å‚¾å‘ï¼ˆPosition biasï¼‰ã‚’è£œæ­£ã›ãšã«
        è©•ä¾¡ã™ã‚‹ã¨ã€å®Ÿéš›ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•ã‚’éå¤§è©•ä¾¡ã™ã‚‹ã€‚

        Args:
            recommendations_list: ãƒ¡ãƒ³ãƒãƒ¼ã”ã¨ã®æ¨è–¦çµæœãƒªã‚¹ãƒˆ
            actual_acquired_list: ãƒ¡ãƒ³ãƒãƒ¼ã”ã¨ã®å®Ÿéš›ã®ç¿’å¾—åŠ›é‡ãƒªã‚¹ãƒˆ

        Returns:
            {
                'position_bias_score': float,  # ä½ç½®ãƒã‚¤ã‚¢ã‚¹ã‚¹ã‚³ã‚¢ï¼ˆ0-1ã€é«˜ã„ã»ã©ãƒã‚¤ã‚¢ã‚¹å¤§ï¼‰
                'top3_hit_ratio': float,  # Top-3ãƒ’ãƒƒãƒˆç‡
                'bottom_half_hit_ratio': float,  # ä¸‹åŠåˆ†ãƒ’ãƒƒãƒˆç‡
                'bias_ratio': float,  # Top-3 / Bottom-half ã®æ¯”ç‡ï¼ˆé«˜ã„ã»ã©ãƒã‚¤ã‚¢ã‚¹å¤§ï¼‰
            }

        Reference:
            Joachims et al. (2017): Unbiased Learning-to-Rank with Biased Feedback
        """
        if not recommendations_list or len(recommendations_list) == 0:
            return {
                'position_bias_score': 0.0,
                'top3_hit_ratio': 0.0,
                'bottom_half_hit_ratio': 0.0,
                'bias_ratio': 0.0,
            }

        top3_hits = 0
        bottom_half_hits = 0
        top3_total = 0
        bottom_half_total = 0

        for recommendations, actual_acquired in zip(recommendations_list, actual_acquired_list):
            if len(recommendations) == 0:
                continue

            actual_set = set(actual_acquired)
            k = len(recommendations)
            half_k = k // 2

            # Top-3ã®ãƒ’ãƒƒãƒˆ
            for i, rec in enumerate(recommendations[:3]):
                top3_total += 1
                if rec.competence_code in actual_set:
                    top3_hits += 1

            # ä¸‹åŠåˆ†ã®ãƒ’ãƒƒãƒˆ
            for i, rec in enumerate(recommendations[half_k:]):
                bottom_half_total += 1
                if rec.competence_code in actual_set:
                    bottom_half_hits += 1

        # Top-3ãƒ’ãƒƒãƒˆç‡
        top3_hit_ratio = top3_hits / top3_total if top3_total > 0 else 0.0

        # ä¸‹åŠåˆ†ãƒ’ãƒƒãƒˆç‡
        bottom_half_hit_ratio = bottom_half_hits / bottom_half_total if bottom_half_total > 0 else 0.0

        # ãƒã‚¤ã‚¢ã‚¹æ¯”ç‡ï¼ˆTop-3 / Bottom-halfï¼‰
        # ç†æƒ³çš„ã«ã¯1.0ã«è¿‘ã„ï¼ˆä½ç½®ãƒã‚¤ã‚¢ã‚¹ãªã—ï¼‰
        # å¤§ãã„ã»ã©ä½ç½®ãƒã‚¤ã‚¢ã‚¹ãŒå¼·ã„
        if bottom_half_hit_ratio > 0:
            bias_ratio = top3_hit_ratio / bottom_half_hit_ratio
        else:
            bias_ratio = 0.0

        # ä½ç½®ãƒã‚¤ã‚¢ã‚¹ã‚¹ã‚³ã‚¢ï¼ˆ0-1ã€é«˜ã„ã»ã©ãƒã‚¤ã‚¢ã‚¹å¤§ï¼‰
        # bias_ratioã‚’æ­£è¦åŒ–ï¼ˆ1.0ãŒç†æƒ³ã€å¤§ãã„ã»ã©ãƒã‚¤ã‚¢ã‚¹å¤§ï¼‰
        if bias_ratio >= 1.0:
            position_bias_score = min(1.0, (bias_ratio - 1.0) / 4.0)  # 5å€ä»¥ä¸Šã§1.0
        else:
            position_bias_score = 0.0

        return {
            'position_bias_score': position_bias_score,
            'top3_hit_ratio': top3_hit_ratio,
            'bottom_half_hit_ratio': bottom_half_hit_ratio,
            'bias_ratio': bias_ratio,
        }

    def evaluate_filter_bubble(
        self,
        train_data: pd.DataFrame,
        recommendations_list: List[List],
        competence_master: pd.DataFrame,
    ) -> Dict[str, float]:
        """
        Filter Bubbleï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒãƒ–ãƒ«ï¼‰ã‚’è©•ä¾¡

        æ—¢å­˜ã‚¹ã‚­ãƒ«ã¨ä¼¼ãŸã‚‚ã®ã°ã‹ã‚Šæ¨è–¦ã—ã¦ã„ãªã„ã‹ã‚’è©•ä¾¡ã€‚
        å¥å…¨ãªæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã¯æ–°ã—ã„ã‚«ãƒ†ã‚´ãƒªãƒ¼ã®ã‚¹ã‚­ãƒ«ã‚‚ææ¡ˆã™ã¹ãã€‚

        Args:
            train_data: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼ˆæ—¢ç¿’å¾—ã‚¹ã‚­ãƒ«ï¼‰
            recommendations_list: ãƒ¡ãƒ³ãƒãƒ¼ã”ã¨ã®æ¨è–¦çµæœãƒªã‚¹ãƒˆ
            competence_master: åŠ›é‡ãƒã‚¹ã‚¿ï¼ˆã‚«ãƒ†ã‚´ãƒªãƒ¼æƒ…å ±ï¼‰

        Returns:
            {
                'filter_bubble_score': float,  # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒãƒ–ãƒ«ã‚¹ã‚³ã‚¢ï¼ˆ0-1ã€é«˜ã„ã»ã©å•é¡Œï¼‰
                'avg_category_overlap': float,  # ã‚«ãƒ†ã‚´ãƒªãƒ¼é‡è¤‡ç‡ï¼ˆ0-1ï¼‰
                'avg_type_overlap': float,  # ã‚¿ã‚¤ãƒ—é‡è¤‡ç‡ï¼ˆ0-1ï¼‰
                'new_category_ratio': float,  # æ–°ã‚«ãƒ†ã‚´ãƒªãƒ¼æ¨è–¦ç‡ï¼ˆ0-1ã€é«˜ã„ã»ã©å¥å…¨ï¼‰
            }

        Reference:
            Pariser (2011): The Filter Bubble
        """
        if not recommendations_list or len(recommendations_list) == 0:
            return {
                'filter_bubble_score': 0.0,
                'avg_category_overlap': 0.0,
                'avg_type_overlap': 0.0,
                'new_category_ratio': 0.0,
            }

        # åŠ›é‡ãƒã‚¹ã‚¿ã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒ»ã‚¿ã‚¤ãƒ—æƒ…å ±ã‚’å–å¾—
        competence_info = {}
        for _, row in competence_master.iterrows():
            competence_info[row['åŠ›é‡ã‚³ãƒ¼ãƒ‰']] = {
                'category': row.get('ã‚«ãƒ†ã‚´ãƒªãƒ¼', 'Unknown'),
                'type': row.get('åŠ›é‡ã‚¿ã‚¤ãƒ—', 'Unknown'),
            }

        category_overlaps = []
        type_overlaps = []
        new_category_counts = []

        # ãƒ¡ãƒ³ãƒãƒ¼ã”ã¨ã«åˆ†æ
        member_codes = train_data['ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰'].unique()
        for member_code, recommendations in zip(member_codes, recommendations_list):
            if len(recommendations) == 0:
                continue

            # ã“ã®ãƒ¡ãƒ³ãƒãƒ¼ã®æ—¢ç¿’å¾—ã‚¹ã‚­ãƒ«ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒ»ã‚¿ã‚¤ãƒ—
            member_train = train_data[train_data['ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰'] == member_code]
            acquired_categories = set()
            acquired_types = set()

            for comp_code in member_train['åŠ›é‡ã‚³ãƒ¼ãƒ‰'].unique():
                info = competence_info.get(comp_code, {})
                acquired_categories.add(info.get('category', 'Unknown'))
                acquired_types.add(info.get('type', 'Unknown'))

            # æ¨è–¦ã‚¹ã‚­ãƒ«ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒ»ã‚¿ã‚¤ãƒ—
            recommended_categories = set()
            recommended_types = set()

            for rec in recommendations:
                info = competence_info.get(rec.competence_code, {})
                recommended_categories.add(info.get('category', 'Unknown'))
                recommended_types.add(info.get('type', 'Unknown'))

            # é‡è¤‡ç‡ã‚’è¨ˆç®—
            category_overlap = 0.0
            if len(recommended_categories) > 0:
                category_overlap = len(acquired_categories & recommended_categories) / len(recommended_categories)

            type_overlap = 0.0
            if len(recommended_types) > 0:
                type_overlap = len(acquired_types & recommended_types) / len(recommended_types)

            category_overlaps.append(category_overlap)
            type_overlaps.append(type_overlap)

            # æ–°ã‚«ãƒ†ã‚´ãƒªãƒ¼æ¨è–¦æ•°
            new_categories = recommended_categories - acquired_categories
            new_category_ratio = len(new_categories) / len(recommended_categories) if len(recommended_categories) > 0 else 0.0
            new_category_counts.append(new_category_ratio)

        # å¹³å‡ã‚’è¨ˆç®—
        avg_category_overlap = np.mean(category_overlaps) if category_overlaps else 0.0
        avg_type_overlap = np.mean(type_overlaps) if type_overlaps else 0.0
        new_category_ratio = np.mean(new_category_counts) if new_category_counts else 0.0

        # Filter Bubble Score: é‡è¤‡ç‡ãŒé«˜ã„ã»ã©ãƒãƒ–ãƒ«ï¼ˆå•é¡Œï¼‰
        filter_bubble_score = (avg_category_overlap + avg_type_overlap) / 2.0

        return {
            'filter_bubble_score': filter_bubble_score,
            'avg_category_overlap': avg_category_overlap,
            'avg_type_overlap': avg_type_overlap,
            'new_category_ratio': new_category_ratio,
        }

    def calculate_intra_list_diversity(
        self,
        recommendations_list: List[List],
        competence_master: pd.DataFrame,
    ) -> float:
        """
        Intra-List Diversity (ILD)ã‚’è¨ˆç®—

        ãƒªã‚¹ãƒˆå†…ã®æ¨è–¦ã‚¹ã‚­ãƒ«åŒå£«ã®å¤šæ§˜æ€§ã‚’æ¸¬å®šã€‚
        é«˜ã„ã»ã©æ¨è–¦ãŒå¤šæ§˜ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼æº€è¶³åº¦å‘ä¸Šï¼‰ã€‚

        Args:
            recommendations_list: ãƒ¡ãƒ³ãƒãƒ¼ã”ã¨ã®æ¨è–¦çµæœãƒªã‚¹ãƒˆ
            competence_master: åŠ›é‡ãƒã‚¹ã‚¿ï¼ˆã‚«ãƒ†ã‚´ãƒªãƒ¼æƒ…å ±ï¼‰

        Returns:
            ILDã‚¹ã‚³ã‚¢ï¼ˆ0-1ã€é«˜ã„ã»ã©å¤šæ§˜ï¼‰

        Reference:
            Vargas & Castells (2011): Rank and Relevance in Novelty and Diversity Metrics
        """
        if not recommendations_list or len(recommendations_list) == 0:
            return 0.0

        # åŠ›é‡ãƒã‚¹ã‚¿ã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªãƒ¼æƒ…å ±ã‚’å–å¾—
        competence_categories = {}
        for _, row in competence_master.iterrows():
            competence_categories[row['åŠ›é‡ã‚³ãƒ¼ãƒ‰']] = row.get('ã‚«ãƒ†ã‚´ãƒªãƒ¼', 'Unknown')

        ild_scores = []

        for recommendations in recommendations_list:
            if len(recommendations) <= 1:
                continue

            # ãƒªã‚¹ãƒˆå†…ã®å„ãƒšã‚¢ã®éé¡ä¼¼åº¦ã‚’è¨ˆç®—
            dissimilarities = []
            n = len(recommendations)

            for i in range(n):
                for j in range(i + 1, n):
                    code_i = recommendations[i].competence_code
                    code_j = recommendations[j].competence_code

                    category_i = competence_categories.get(code_i, 'Unknown')
                    category_j = competence_categories.get(code_j, 'Unknown')

                    # ç•°ãªã‚‹ã‚«ãƒ†ã‚´ãƒªãƒ¼ = éé¡ä¼¼åº¦1.0ã€åŒã˜ = 0.0
                    dissimilarity = 1.0 if category_i != category_j else 0.0
                    dissimilarities.append(dissimilarity)

            # å¹³å‡éé¡ä¼¼åº¦ = ILD
            if dissimilarities:
                ild = np.mean(dissimilarities)
                ild_scores.append(ild)

        return np.mean(ild_scores) if ild_scores else 0.0

    def calculate_serendipity(
        self,
        recommendations_list: List[List],
        train_data: pd.DataFrame,
        member_competence: pd.DataFrame,
        competence_master: pd.DataFrame,
    ) -> float:
        """
        Serendipityï¼ˆæ„å¤–æ€§ï¼‰ã‚’è¨ˆç®—

        æ„å¤–ã§ã‚ã‚ŠãªãŒã‚‰æœ‰ç”¨ãªæ¨è–¦ï¼ˆã‚»ãƒ¬ãƒ³ãƒ‡ã‚£ãƒ”ãƒ†ã‚£ï¼‰ã‚’æ¸¬å®šã€‚
        - äººæ°—åº¦ãŒä½ã„ï¼ˆæ„å¤–ï¼‰
        - æ—¢ç¿’å¾—ã‚¹ã‚­ãƒ«ã¨ç•°ãªã‚‹ã‚«ãƒ†ã‚´ãƒªãƒ¼ï¼ˆæ„å¤–ï¼‰
        - ã‹ã¤å®Ÿéš›ã«ç¿’å¾—ã•ã‚ŒãŸï¼ˆæœ‰ç”¨ï¼‰

        Args:
            recommendations_list: ãƒ¡ãƒ³ãƒãƒ¼ã”ã¨ã®æ¨è–¦çµæœãƒªã‚¹ãƒˆ
            train_data: è¨“ç·´ãƒ‡ãƒ¼ã‚¿
            member_competence: å…¨ãƒ¡ãƒ³ãƒãƒ¼ç¿’å¾—ãƒ‡ãƒ¼ã‚¿ï¼ˆäººæ°—åº¦è¨ˆç®—ç”¨ï¼‰
            competence_master: åŠ›é‡ãƒã‚¹ã‚¿

        Returns:
            Serendipityã‚¹ã‚³ã‚¢ï¼ˆ0-1ã€é«˜ã„ã»ã©æ„å¤–æ€§ãŒé«˜ã„ï¼‰

        Reference:
            Ge et al. (2010): Beyond Accuracy: Evaluating Recommender Systems by Coverage and Serendipity
        """
        if not recommendations_list or len(recommendations_list) == 0:
            return 0.0

        # äººæ°—åº¦ã‚’è¨ˆç®—
        competence_counts = member_competence['åŠ›é‡ã‚³ãƒ¼ãƒ‰'].value_counts()
        total_members = member_competence['ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰'].nunique()
        competence_popularity = {
            code: count / total_members
            for code, count in competence_counts.items()
        }

        # ã‚«ãƒ†ã‚´ãƒªãƒ¼æƒ…å ±
        competence_categories = {}
        for _, row in competence_master.iterrows():
            competence_categories[row['åŠ›é‡ã‚³ãƒ¼ãƒ‰']] = row.get('ã‚«ãƒ†ã‚´ãƒªãƒ¼', 'Unknown')

        serendipity_scores = []

        # ãƒ¡ãƒ³ãƒãƒ¼ã”ã¨ã«åˆ†æ
        member_codes = train_data['ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰'].unique()
        for member_code, recommendations in zip(member_codes, recommendations_list):
            if len(recommendations) == 0:
                continue

            # ã“ã®ãƒ¡ãƒ³ãƒãƒ¼ã®æ—¢ç¿’å¾—ã‚«ãƒ†ã‚´ãƒªãƒ¼
            member_train = train_data[train_data['ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰'] == member_code]
            acquired_categories = set()
            for comp_code in member_train['åŠ›é‡ã‚³ãƒ¼ãƒ‰'].unique():
                acquired_categories.add(competence_categories.get(comp_code, 'Unknown'))

            # å„æ¨è–¦ã®ã‚»ãƒ¬ãƒ³ãƒ‡ã‚£ãƒ”ãƒ†ã‚£ã‚’è¨ˆç®—
            for rec in recommendations:
                popularity = competence_popularity.get(rec.competence_code, 0.0)
                category = competence_categories.get(rec.competence_code, 'Unknown')

                # æ„å¤–æ€§ = ä½äººæ°—åº¦ Ã— ç•°ã‚«ãƒ†ã‚´ãƒªãƒ¼
                # äººæ°—åº¦ãŒä½ã„ã»ã©æ„å¤–ï¼ˆ1 - popularityï¼‰
                unexpectedness = 1.0 - popularity

                # ç•°ãªã‚‹ã‚«ãƒ†ã‚´ãƒªãƒ¼ãªã‚‰æ„å¤–æ€§åŠ ç®—
                if category not in acquired_categories:
                    unexpectedness *= 1.5  # ãƒ–ãƒ¼ã‚¹ãƒˆ

                serendipity_scores.append(min(1.0, unexpectedness))

        return np.mean(serendipity_scores) if serendipity_scores else 0.0

    def calculate_personalization(
        self,
        recommendations_list: List[List],
    ) -> float:
        """
        Personalizationï¼ˆå€‹äººåŒ–åº¦ï¼‰ã‚’è¨ˆç®—

        ãƒ¦ãƒ¼ã‚¶ãƒ¼é–“ã®æ¨è–¦ã®å·®ç•°ã‚’æ¸¬å®šã€‚
        é«˜ã„ã»ã©å€‹äººåŒ–ã•ã‚Œã¦ã„ã‚‹ï¼ˆåŒã˜æ¨è–¦ã‚’å…¨å“¡ã«å‡ºã—ã¦ã„ãªã„ï¼‰ã€‚

        Args:
            recommendations_list: ãƒ¡ãƒ³ãƒãƒ¼ã”ã¨ã®æ¨è–¦çµæœãƒªã‚¹ãƒˆ

        Returns:
            Personalizationã‚¹ã‚³ã‚¢ï¼ˆ0-1ã€é«˜ã„ã»ã©å€‹äººåŒ–ã•ã‚Œã¦ã„ã‚‹ï¼‰

        Formula:
            personalization = 1 - avg_jaccard_similarity(recommendations)

        Reference:
            Adomavicius & Tuzhilin (2005): Toward the Next Generation of Recommender Systems
        """
        if not recommendations_list or len(recommendations_list) < 2:
            return 0.0

        # å„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ¨è–¦ã‚’ã‚»ãƒƒãƒˆã«å¤‰æ›
        recommendation_sets = []
        for recommendations in recommendations_list:
            rec_set = set([rec.competence_code for rec in recommendations])
            recommendation_sets.append(rec_set)

        # å…¨ãƒšã‚¢ã®Jaccardé¡ä¼¼åº¦ã‚’è¨ˆç®—
        jaccard_similarities = []
        n = len(recommendation_sets)

        for i in range(n):
            for j in range(i + 1, n):
                set_i = recommendation_sets[i]
                set_j = recommendation_sets[j]

                if len(set_i) == 0 and len(set_j) == 0:
                    continue

                # Jaccardé¡ä¼¼åº¦
                intersection = len(set_i & set_j)
                union = len(set_i | set_j)

                if union > 0:
                    jaccard = intersection / union
                    jaccard_similarities.append(jaccard)

        # Personalization = 1 - avg(Jaccard)
        avg_jaccard = np.mean(jaccard_similarities) if jaccard_similarities else 0.0
        personalization = 1.0 - avg_jaccard

        return personalization

    def calculate_calibration(
        self,
        train_data: pd.DataFrame,
        recommendations_list: List[List],
        competence_master: pd.DataFrame,
    ) -> Dict[str, float]:
        """
        Calibrationï¼ˆã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰ã‚’è¨ˆç®—

        æ¨è–¦åˆ†å¸ƒ vs ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®èˆˆå‘³åˆ†å¸ƒã®KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã‚’æ¸¬å®šã€‚
        ä½ã„ã»ã©ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®èˆˆå‘³ã‚’å¿ å®Ÿã«åæ˜ ã—ã¦ã„ã‚‹ã€‚

        Args:
            train_data: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã®èˆˆå‘³ã‚’æ¨å®šï¼‰
            recommendations_list: ãƒ¡ãƒ³ãƒãƒ¼ã”ã¨ã®æ¨è–¦çµæœãƒªã‚¹ãƒˆ
            competence_master: åŠ›é‡ãƒã‚¹ã‚¿ï¼ˆã‚«ãƒ†ã‚´ãƒªãƒ¼æƒ…å ±ï¼‰

        Returns:
            {
                'kl_divergence': float,  # KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ï¼ˆä½ã„ã»ã©è‰¯ã„ï¼‰
                'category_calibration': float,  # ã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒ¬ãƒ™ãƒ«ã®ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
            }

        Reference:
            Steck (2018): Calibrated Recommendations
        """
        if not recommendations_list or len(recommendations_list) == 0:
            return {
                'kl_divergence': 0.0,
                'category_calibration': 0.0,
            }

        # ã‚«ãƒ†ã‚´ãƒªãƒ¼æƒ…å ±
        competence_categories = {}
        for _, row in competence_master.iterrows():
            competence_categories[row['åŠ›é‡ã‚³ãƒ¼ãƒ‰']] = row.get('ã‚«ãƒ†ã‚´ãƒªãƒ¼', 'Unknown')

        kl_divergences = []
        category_calibrations = []

        # ãƒ¡ãƒ³ãƒãƒ¼ã”ã¨ã«åˆ†æ
        member_codes = train_data['ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰'].unique()
        for member_code, recommendations in zip(member_codes, recommendations_list):
            if len(recommendations) == 0:
                continue

            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®èˆˆå‘³åˆ†å¸ƒï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ†å¸ƒï¼‰
            member_train = train_data[train_data['ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰'] == member_code]
            user_category_counts = Counter()

            for comp_code in member_train['åŠ›é‡ã‚³ãƒ¼ãƒ‰'].unique():
                category = competence_categories.get(comp_code, 'Unknown')
                user_category_counts[category] += 1

            # æ¨è–¦åˆ†å¸ƒï¼ˆæ¨è–¦ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ†å¸ƒï¼‰
            rec_category_counts = Counter()
            for rec in recommendations:
                category = competence_categories.get(rec.competence_code, 'Unknown')
                rec_category_counts[category] += 1

            # å…¨ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚’å–å¾—
            all_categories = set(user_category_counts.keys()) | set(rec_category_counts.keys())

            if len(all_categories) == 0:
                continue

            # ç¢ºç‡åˆ†å¸ƒã«å¤‰æ›
            user_total = sum(user_category_counts.values())
            rec_total = sum(rec_category_counts.values())

            user_probs = []
            rec_probs = []

            for category in all_categories:
                user_prob = user_category_counts[category] / user_total if user_total > 0 else 0.0
                rec_prob = rec_category_counts[category] / rec_total if rec_total > 0 else 0.0

                user_probs.append(user_prob)
                rec_probs.append(rec_prob)

            # KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã‚’è¨ˆç®—: KL(P || Q) = Î£ p(x) log(p(x) / q(x))
            kl_div = 0.0
            for p, q in zip(user_probs, rec_probs):
                if p > 0 and q > 0:
                    kl_div += p * np.log(p / q)
                elif p > 0 and q == 0:
                    # q=0ã®å ´åˆã¯éå¸¸ã«å¤§ããªãƒšãƒŠãƒ«ãƒ†ã‚£
                    kl_div += p * 10.0

            kl_divergences.append(kl_div)

            # Category Calibration: ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ†å¸ƒã¨æ¨è–¦åˆ†å¸ƒã®å·®ã®çµ¶å¯¾å€¤
            category_calib = sum(abs(p - q) for p, q in zip(user_probs, rec_probs)) / 2.0
            category_calibrations.append(category_calib)

        return {
            'kl_divergence': np.mean(kl_divergences) if kl_divergences else 0.0,
            'category_calibration': np.mean(category_calibrations) if category_calibrations else 0.0,
        }

    def comprehensive_evaluation(
        self,
        train_data: pd.DataFrame,
        test_data: pd.DataFrame,
        competence_master: pd.DataFrame,
        top_k: int = 10,
        member_sample: Optional[List[str]] = None,
        include_all_metrics: bool = True,
    ) -> ComprehensiveEvaluationResult:
        """
        åŒ…æ‹¬çš„ãªè©•ä¾¡ã‚’å®Ÿè¡Œ

        ç²¾åº¦ã€ãƒã‚¤ã‚¢ã‚¹ã€å¤šæ§˜æ€§ã€ã‚«ãƒãƒ¬ãƒƒã‚¸ã€ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä¸€åº¦ã«è©•ä¾¡ã€‚

        Args:
            train_data: è¨“ç·´ãƒ‡ãƒ¼ã‚¿
            test_data: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
            competence_master: åŠ›é‡ãƒã‚¹ã‚¿
            top_k: Top-Kæ¨è–¦
            member_sample: è©•ä¾¡å¯¾è±¡ãƒ¡ãƒ³ãƒãƒ¼ï¼ˆNoneã§å…¨å“¡ï¼‰
            include_all_metrics: å…¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—ã™ã‚‹ã‹ï¼ˆé«˜ã‚³ã‚¹ãƒˆï¼‰

        Returns:
            ComprehensiveEvaluationResult

        Example:
            >>> evaluator = AdvancedMetricsEvaluator()
            >>> result = evaluator.comprehensive_evaluation(
            ...     train_data=train_df,
            ...     test_data=test_df,
            ...     competence_master=master_df,
            ...     top_k=10
            ... )
            >>> print(f"Overall Score: {result.overall_score:.3f}")
            >>> evaluator.print_comprehensive_report(result)
        """
        print(f"\n{'='*80}")
        print(f"åŒ…æ‹¬çš„è©•ä¾¡é–‹å§‹ï¼ˆGAFAãƒ¬ãƒ™ãƒ«ï¼‰")
        print(f"{'='*80}\n")

        # 1. ç²¾åº¦è©•ä¾¡ï¼ˆæ—¢å­˜ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼‰
        print("[1/5] ç²¾åº¦è©•ä¾¡ï¼ˆPrecision, Recall, NDCG, MRR, MAPï¼‰...")
        accuracy_metrics = self.evaluate_with_diversity(
            train_data=train_data,
            test_data=test_data,
            competence_master=competence_master,
            top_k=top_k,
            member_sample=member_sample,
        )

        # æ¨è–¦çµæœã‚’åé›†ï¼ˆä»¥é™ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ç”¨ï¼‰
        if member_sample is None:
            member_sample = test_data['ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰'].unique().tolist()

        recommendations_list, actual_acquired_list = self._collect_recommendations(
            train_data=train_data,
            test_data=test_data,
            competence_master=competence_master,
            member_sample=member_sample,
            top_k=top_k,
        )

        # 2. ãƒã‚¤ã‚¢ã‚¹è©•ä¾¡
        print("[2/5] ãƒã‚¤ã‚¢ã‚¹è©•ä¾¡ï¼ˆPopularity, Position, Filter bubbleï¼‰...")
        bias_metrics = {}

        # Popularity Bias
        popularity_bias = self.evaluate_popularity_bias(
            recommendations_list=recommendations_list,
            member_competence=train_data,
        )
        bias_metrics.update(popularity_bias)

        # Position Bias
        position_bias = self.evaluate_position_bias(
            recommendations_list=recommendations_list,
            actual_acquired_list=actual_acquired_list,
        )
        bias_metrics.update(position_bias)

        # Filter Bubble
        filter_bubble = self.evaluate_filter_bubble(
            train_data=train_data,
            recommendations_list=recommendations_list,
            competence_master=competence_master,
        )
        bias_metrics.update(filter_bubble)

        # 3. å¤šæ§˜æ€§è©•ä¾¡
        print("[3/5] å¤šæ§˜æ€§è©•ä¾¡ï¼ˆILD, Serendipity, Personalizationï¼‰...")
        diversity_metrics = {}

        # ILD
        ild = self.calculate_intra_list_diversity(
            recommendations_list=recommendations_list,
            competence_master=competence_master,
        )
        diversity_metrics['intra_list_diversity'] = ild

        # Serendipity
        serendipity = self.calculate_serendipity(
            recommendations_list=recommendations_list,
            train_data=train_data,
            member_competence=train_data,
            competence_master=competence_master,
        )
        diversity_metrics['serendipity'] = serendipity

        # Personalization
        personalization = self.calculate_personalization(
            recommendations_list=recommendations_list,
        )
        diversity_metrics['personalization'] = personalization

        # 4. ã‚«ãƒãƒ¬ãƒƒã‚¸è©•ä¾¡ï¼ˆæ—¢ã«accuracy_metricsã«å«ã¾ã‚Œã¦ã„ã‚‹ï¼‰
        print("[4/5] ã‚«ãƒãƒ¬ãƒƒã‚¸è©•ä¾¡...")
        coverage_metrics = {
            'catalog_coverage': accuracy_metrics.get('catalog_coverage', 0.0),
            'total_unique_recommended': accuracy_metrics.get('total_unique_recommended', 0),
        }

        # 5. ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è©•ä¾¡
        print("[5/5] ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è©•ä¾¡ï¼ˆKL divergenceï¼‰...")
        calibration_metrics = self.calculate_calibration(
            train_data=train_data,
            recommendations_list=recommendations_list,
            competence_master=competence_master,
        )

        # ç·åˆã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆé‡ã¿ä»˜ãå¹³å‡ï¼‰
        overall_score = self._calculate_overall_score(
            accuracy_metrics=accuracy_metrics,
            bias_metrics=bias_metrics,
            diversity_metrics=diversity_metrics,
            coverage_metrics=coverage_metrics,
            calibration_metrics=calibration_metrics,
            top_k=top_k,
        )

        result = ComprehensiveEvaluationResult(
            accuracy_metrics=accuracy_metrics,
            bias_metrics=bias_metrics,
            diversity_metrics=diversity_metrics,
            coverage_metrics=coverage_metrics,
            calibration_metrics=calibration_metrics,
            overall_score=overall_score,
        )

        print(f"\n{'='*80}")
        print(f"åŒ…æ‹¬çš„è©•ä¾¡å®Œäº†")
        print(f"{'='*80}\n")

        return result

    def _collect_recommendations(
        self,
        train_data: pd.DataFrame,
        test_data: pd.DataFrame,
        competence_master: pd.DataFrame,
        member_sample: List[str],
        top_k: int,
    ) -> Tuple[List[List], List[List[str]]]:
        """æ¨è–¦çµæœã¨æ­£è§£ãƒ‡ãƒ¼ã‚¿ã‚’åé›†"""
        # MLãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã®æº–å‚™
        if self.recommender is None:
            from ..ml.ml_recommender import MLRecommender

            member_codes = train_data['ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰'].unique()
            members_data = pd.DataFrame({
                'ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰': member_codes,
                'ãƒ¡ãƒ³ãƒãƒ¼å': [f'ãƒ¡ãƒ³ãƒãƒ¼{code}' for code in member_codes],
                'å½¹è·': ['æœªè¨­å®š'] * len(member_codes),
                'è·èƒ½ç­‰ç´š': ['æœªè¨­å®š'] * len(member_codes),
            })

            n_members = len(train_data['ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰'].unique())
            n_competences = len(train_data['åŠ›é‡ã‚³ãƒ¼ãƒ‰'].unique())
            safe_n_components = min(20, n_members, n_competences)

            recommender = MLRecommender.build(
                member_competence=train_data,
                competence_master=competence_master,
                member_master=members_data,
                use_preprocessing=False,
                use_tuning=False,
                n_components=safe_n_components,
            )
        else:
            recommender = self.recommender

        recommendations_list = []
        actual_acquired_list = []

        for member_code in member_sample:
            # æ­£è§£ãƒ‡ãƒ¼ã‚¿
            actual_acquired = test_data[test_data['ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰'] == member_code]['åŠ›é‡ã‚³ãƒ¼ãƒ‰'].unique().tolist()

            if len(actual_acquired) == 0:
                continue

            try:
                # æ¨è–¦ç”Ÿæˆ
                recommendations = recommender.recommend(
                    member_code=member_code,
                    top_n=top_k,
                    use_diversity=False
                )

                if len(recommendations) > 0:
                    recommendations_list.append(recommendations)
                    actual_acquired_list.append(actual_acquired)

            except Exception:
                continue

        return recommendations_list, actual_acquired_list

    def _calculate_overall_score(
        self,
        accuracy_metrics: Dict[str, float],
        bias_metrics: Dict[str, float],
        diversity_metrics: Dict[str, float],
        coverage_metrics: Dict[str, float],
        calibration_metrics: Dict[str, float],
        top_k: int,
    ) -> float:
        """
        ç·åˆã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆé‡ã¿ä»˜ãå¹³å‡ï¼‰

        é‡ã¿:
        - Accuracy: 40%
        - Diversity: 20%
        - Coverage: 15%
        - Bias (inverse): 15%
        - Calibration (inverse): 10%
        """
        # Accuracy (0-1)
        accuracy_score = (
            accuracy_metrics.get(f'precision@{top_k}', 0.0) * 0.3
            + accuracy_metrics.get(f'recall@{top_k}', 0.0) * 0.3
            + accuracy_metrics.get(f'ndcg@{top_k}', 0.0) * 0.4
        )

        # Diversity (0-1)
        diversity_score = (
            diversity_metrics.get('intra_list_diversity', 0.0) * 0.4
            + diversity_metrics.get('serendipity', 0.0) * 0.3
            + diversity_metrics.get('personalization', 0.0) * 0.3
        )

        # Coverage (0-1)
        coverage_score = coverage_metrics.get('catalog_coverage', 0.0)

        # Bias (inverse, 0-1)
        # ä½ã„ã»ã©è‰¯ã„ã®ã§åè»¢
        bias_score = 1.0 - (
            bias_metrics.get('popularity_bias', 0.0) * 0.3
            + bias_metrics.get('position_bias_score', 0.0) * 0.3
            + bias_metrics.get('filter_bubble_score', 0.0) * 0.4
        )

        # Calibration (inverse, 0-1)
        # KL divergenceã¯ä½ã„ã»ã©è‰¯ã„ï¼ˆ0-10ç¨‹åº¦ã‚’æƒ³å®šï¼‰
        kl_div = calibration_metrics.get('kl_divergence', 0.0)
        calibration_score = max(0.0, 1.0 - kl_div / 10.0)

        # ç·åˆã‚¹ã‚³ã‚¢
        overall = (
            accuracy_score * 0.40
            + diversity_score * 0.20
            + coverage_score * 0.15
            + bias_score * 0.15
            + calibration_score * 0.10
        )

        return overall

    def print_comprehensive_report(self, result: ComprehensiveEvaluationResult, top_k: int = 10):
        """
        åŒ…æ‹¬çš„è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆã‚’è¡¨ç¤º

        Args:
            result: ComprehensiveEvaluationResult
            top_k: Top-Kï¼ˆè¡¨ç¤ºç”¨ï¼‰
        """
        print(f"\n{'='*80}")
        print(f"ğŸ“Š åŒ…æ‹¬çš„è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆï¼ˆGAFAãƒ¬ãƒ™ãƒ«ï¼‰")
        print(f"{'='*80}\n")

        # ç·åˆã‚¹ã‚³ã‚¢
        print(f"ğŸ¯ ç·åˆã‚¹ã‚³ã‚¢: {result.overall_score:.3f} / 1.000")
        print(f"{'='*80}\n")

        # 1. ç²¾åº¦è©•ä¾¡
        print(f"ã€1. ç²¾åº¦è©•ä¾¡ï¼ˆAccuracy Metricsï¼‰ã€‘")
        print(f"  Precision@{top_k}:  {result.accuracy_metrics.get(f'precision@{top_k}', 0.0):.4f}")
        print(f"  Recall@{top_k}:     {result.accuracy_metrics.get(f'recall@{top_k}', 0.0):.4f}")
        print(f"  NDCG@{top_k}:       {result.accuracy_metrics.get(f'ndcg@{top_k}', 0.0):.4f}")
        print(f"  F1@{top_k}:         {result.accuracy_metrics.get(f'f1@{top_k}', 0.0):.4f}")
        print(f"  Hit Rate:       {result.accuracy_metrics.get('hit_rate', 0.0):.4f}\n")

        # 2. ãƒã‚¤ã‚¢ã‚¹è©•ä¾¡
        print(f"ã€2. ãƒã‚¤ã‚¢ã‚¹è©•ä¾¡ï¼ˆBias Metricsï¼‰ã€‘")
        print(f"  Popularity Bias:  {result.bias_metrics.get('popularity_bias', 0.0):.4f}  (ä½ã„ã»ã©å¥å…¨)")
        print(f"  Tail Ratio:       {result.bias_metrics.get('tail_ratio', 0.0):.4f}  (é«˜ã„ã»ã©å¥å…¨)")
        print(f"  Position Bias:    {result.bias_metrics.get('position_bias_score', 0.0):.4f}  (ä½ã„ã»ã©å¥å…¨)")
        print(f"  Filter Bubble:    {result.bias_metrics.get('filter_bubble_score', 0.0):.4f}  (ä½ã„ã»ã©å¥å…¨)\n")

        # 3. å¤šæ§˜æ€§è©•ä¾¡
        print(f"ã€3. å¤šæ§˜æ€§è©•ä¾¡ï¼ˆDiversity Metricsï¼‰ã€‘")
        print(f"  ILD:              {result.diversity_metrics.get('intra_list_diversity', 0.0):.4f}  (ãƒªã‚¹ãƒˆå†…å¤šæ§˜æ€§)")
        print(f"  Serendipity:      {result.diversity_metrics.get('serendipity', 0.0):.4f}  (æ„å¤–æ€§)")
        print(f"  Personalization:  {result.diversity_metrics.get('personalization', 0.0):.4f}  (å€‹äººåŒ–åº¦)\n")

        # 4. ã‚«ãƒãƒ¬ãƒƒã‚¸è©•ä¾¡
        print(f"ã€4. ã‚«ãƒãƒ¬ãƒƒã‚¸è©•ä¾¡ï¼ˆCoverage Metricsï¼‰ã€‘")
        print(f"  Catalog Coverage: {result.coverage_metrics.get('catalog_coverage', 0.0):.4f}  (æ¨è–¦ã«å«ã¾ã‚ŒãŸåŠ›é‡ã®å‰²åˆ)")
        print(f"  Unique Items:     {result.coverage_metrics.get('total_unique_recommended', 0)}å€‹\n")

        # 5. ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è©•ä¾¡
        print(f"ã€5. ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è©•ä¾¡ï¼ˆCalibration Metricsï¼‰ã€‘")
        print(f"  KL Divergence:    {result.calibration_metrics.get('kl_divergence', 0.0):.4f}  (ä½ã„ã»ã©è‰¯ã„)")
        print(f"  Category Calib.:  {result.calibration_metrics.get('category_calibration', 0.0):.4f}  (ä½ã„ã»ã©è‰¯ã„)\n")

        print(f"{'='*80}")

        # è¨ºæ–­ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        self._print_diagnostic_messages(result)

    def _print_diagnostic_messages(self, result: ComprehensiveEvaluationResult):
        """è¨ºæ–­ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º"""
        print(f"\nğŸ’¡ è¨ºæ–­ãƒ»æ¨å¥¨äº‹é …:\n")

        issues = []
        recommendations = []

        # Popularity Bias
        if result.bias_metrics.get('popularity_bias', 0.0) > 0.7:
            issues.append("âš ï¸ äººæ°—ã‚¹ã‚­ãƒ«ã‚’éå‰°ã«æ¨è–¦ã—ã¦ã„ã¾ã™ï¼ˆPopularity Biasï¼‰")
            recommendations.append("   â†’ ãƒ­ãƒ³ã‚°ãƒ†ãƒ¼ãƒ«ã‚¹ã‚­ãƒ«ã®é‡ã¿ä»˜ã‘ã‚’å¢—ã‚„ã—ã¦ãã ã•ã„")

        # Position Bias
        if result.bias_metrics.get('position_bias_score', 0.0) > 0.5:
            issues.append("âš ï¸ ãƒ©ãƒ³ã‚­ãƒ³ã‚°ä¸Šä½ã«åã£ãŸè©•ä¾¡ã«ãªã£ã¦ã„ã¾ã™ï¼ˆPosition Biasï¼‰")
            recommendations.append("   â†’ ä½ç½®ãƒã‚¤ã‚¢ã‚¹è£œæ­£ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„")

        # Filter Bubble
        if result.bias_metrics.get('filter_bubble_score', 0.0) > 0.7:
            issues.append("âš ï¸ æ—¢å­˜ã‚¹ã‚­ãƒ«ã¨ä¼¼ãŸã‚‚ã®ã°ã‹ã‚Šæ¨è–¦ã—ã¦ã„ã¾ã™ï¼ˆFilter Bubbleï¼‰")
            recommendations.append("   â†’ æ–°ã‚«ãƒ†ã‚´ãƒªãƒ¼ã®æ¨è–¦é‡ã¿ã‚’å¢—ã‚„ã—ã¦ãã ã•ã„")

        # ILD
        if result.diversity_metrics.get('intra_list_diversity', 0.0) < 0.3:
            issues.append("âš ï¸ ãƒªã‚¹ãƒˆå†…ã®å¤šæ§˜æ€§ãŒä½ã„ã§ã™ï¼ˆILDï¼‰")
            recommendations.append("   â†’ å¤šæ§˜æ€§ã‚’è€ƒæ…®ã—ãŸæ¨è–¦ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å°å…¥ã—ã¦ãã ã•ã„")

        # Personalization
        if result.diversity_metrics.get('personalization', 0.0) < 0.3:
            issues.append("âš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼é–“ã§æ¨è–¦ãŒé¡ä¼¼ã—ã¦ã„ã¾ã™ï¼ˆPersonalizationï¼‰")
            recommendations.append("   â†’ ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³å¼·åº¦ã‚’ä¸Šã’ã¦ãã ã•ã„")

        # Coverage
        if result.coverage_metrics.get('catalog_coverage', 0.0) < 0.2:
            issues.append("âš ï¸ ã‚«ã‚¿ãƒ­ã‚°ã‚«ãƒãƒ¬ãƒƒã‚¸ãŒä½ã„ã§ã™")
            recommendations.append("   â†’ ãƒ­ãƒ³ã‚°ãƒ†ãƒ¼ãƒ«ã‚¢ã‚¤ãƒ†ãƒ ã‚’æ¨è–¦ã™ã‚‹ã‚ˆã†èª¿æ•´ã—ã¦ãã ã•ã„")

        if issues:
            for issue in issues:
                print(issue)
            print()
            for rec in recommendations:
                print(rec)
        else:
            print("âœ… ã™ã¹ã¦ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒå¥å…¨ãªç¯„å›²å†…ã§ã™")

        print()
