"""
ã‚­ãƒ£ãƒªã‚¢æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ  - ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã¨åˆ†æ
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go

from skillnote_recommendation.ml.ml_recommender import MLRecommender


# =========================================================
# ãƒšãƒ¼ã‚¸è¨­å®š
# =========================================================
st.set_page_config(
    page_title="ã‚­ãƒ£ãƒªã‚¢æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ  - ãƒ¢ãƒ‡ãƒ«å­¦ç¿’",
    page_icon="ğŸ¤–",
    layout="wide"
)

st.title("ğŸ¤– ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã¨åˆ†æ")
st.markdown("**ã‚¹ãƒ†ãƒƒãƒ—2**: MLãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ã€å­¦ç¿’çµæœã‚’åˆ†æã—ã¾ã™ã€‚")


# =========================================================
# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒã‚§ãƒƒã‚¯
# =========================================================
if not st.session_state.get("data_loaded", False):
    st.warning("âš ï¸ ã¾ãšãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ãã ã•ã„ã€‚")
    st.info("ğŸ‘‰ ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ã€Œãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã€ãƒšãƒ¼ã‚¸ã«æˆ»ã£ã¦CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
    st.stop()


# =========================================================
# è£œåŠ©é–¢æ•°
# =========================================================
def build_ml_recommender(
    transformed_data: dict,
    use_preprocessing: bool = True,
    use_tuning: bool = False
) -> MLRecommender:
    """
    MLRecommenderã‚’å­¦ç¿’æ¸ˆã¿ã®çŠ¶æ…‹ã§ä½œæˆã™ã‚‹

    Args:
        transformed_data: å¤‰æ›æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿
        use_preprocessing: ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‚’ä½¿ç”¨ã™ã‚‹ã‹
        use_tuning: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ä½¿ç”¨ã™ã‚‹ã‹
    """
    recommender = MLRecommender.build(
        member_competence=transformed_data["member_competence"],
        competence_master=transformed_data["competence_master"],
        member_master=transformed_data["members_clean"],
        use_preprocessing=use_preprocessing,
        use_tuning=use_tuning
    )
    return recommender


# =========================================================
# ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
# =========================================================
st.subheader("ğŸ“ MLãƒ¢ãƒ‡ãƒ«å­¦ç¿’")

if st.session_state.get("model_trained", False):
    st.success("âœ… MLãƒ¢ãƒ‡ãƒ«ã¯æ—¢ã«å­¦ç¿’æ¸ˆã¿ã§ã™ã€‚")

    if st.button("ğŸ”„ ãƒ¢ãƒ‡ãƒ«ã‚’å†å­¦ç¿’ã™ã‚‹"):
        st.session_state.model_trained = False
        st.session_state.ml_recommender = None
        st.rerun()
else:
    st.info("ğŸ“š NMFï¼ˆéè² å€¤è¡Œåˆ—åˆ†è§£ï¼‰ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ¡ãƒ³ãƒãƒ¼ã®åŠ›é‡ç¿’å¾—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã—ã¾ã™ã€‚")

    # å­¦ç¿’ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    with st.expander("âš™ï¸ å­¦ç¿’ã‚ªãƒ—ã‚·ãƒ§ãƒ³", expanded=True):
        col1, col2 = st.columns(2)

        with col1:
            use_preprocessing = st.checkbox(
                "ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‚’ä½¿ç”¨",
                value=True,
                help="å¤–ã‚Œå€¤é™¤å»ã¨æ­£è¦åŒ–ã‚’è¡Œã„ã¾ã™ã€‚å†æ§‹æˆèª¤å·®ã®æ”¹å–„ã«åŠ¹æœçš„ã§ã™ã€‚"
            )

        with col2:
            use_tuning = st.checkbox(
                "ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° (Optuna)",
                value=False,
                help="ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã§ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è‡ªå‹•èª¿æ•´ã—ã¾ã™ã€‚æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ãŒã€æœ€è‰¯ã®ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã§ãã¾ã™ã€‚"
            )

        if use_preprocessing:
            st.markdown("""
            **ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã®å†…å®¹:**
            - å¤–ã‚Œå€¤é™¤å»: åŠ›é‡æ•°ãŒæ¥µç«¯ã«å°‘ãªã„ãƒ¡ãƒ³ãƒãƒ¼/ä¿æœ‰è€…ãŒå°‘ãªã„åŠ›é‡ã‚’é™¤å¤–
            - æ­£è¦åŒ–: Min-Maxã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼ˆ0-1ç¯„å›²ã«æ­£è¦åŒ–ï¼‰
            """)

        if use_tuning:
            st.markdown("""
            **ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®å†…å®¹:**
            - æ¢ç´¢æ–¹æ³•: TPEï¼ˆTree-structured Parzen Estimatorï¼‰ãƒ™ã‚¤ã‚ºæœ€é©åŒ–
            - æ¢ç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: æ½œåœ¨å› å­æ•°ã€æ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€æœ€å¤§ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°ãªã©
            - è©¦è¡Œå›æ•°: 50å›ï¼ˆç´„5-10åˆ†ï¼‰
            """)
            st.warning("â±ï¸ ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã¯5-10åˆ†ç¨‹åº¦ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚")

    # å­¦ç¿’å®Ÿè¡Œãƒœã‚¿ãƒ³
    button_label = "ğŸš€ MLãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚’å®Ÿè¡Œï¼ˆãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚ã‚Šï¼‰" if use_tuning else "ğŸš€ MLãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚’å®Ÿè¡Œ"

    if st.button(button_label, type="primary"):
        with st.spinner("MLãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ä¸­..." if not use_tuning else "ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­..."):
            try:
                ml_recommender = build_ml_recommender(
                    st.session_state.transformed_data,
                    use_preprocessing=use_preprocessing,
                    use_tuning=use_tuning
                )
                st.session_state.ml_recommender = ml_recommender
                st.session_state.model_trained = True
                st.success("âœ… MLãƒ¢ãƒ‡ãƒ«å­¦ç¿’ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
                st.rerun()
            except Exception as e:
                import traceback
                st.error(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {type(e).__name__}: {e}")
                st.code(traceback.format_exc())
                st.info("ãƒ‡ãƒãƒƒã‚°æƒ…å ±:")
                st.write("transformed_data keys:", list(st.session_state.transformed_data.keys()))


# =========================================================
# å­¦ç¿’çµæœã®åˆ†æ
# =========================================================
if st.session_state.get("model_trained", False):
    st.markdown("---")
    st.subheader("ğŸ“Š å­¦ç¿’çµæœã®åˆ†æ")

    recommender = st.session_state.ml_recommender
    mf_model = recommender.mf_model

    # åŸºæœ¬çµ±è¨ˆ
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric("æ½œåœ¨å› å­æ•°", mf_model.n_components)

    with col2:
        st.metric("ãƒ¡ãƒ³ãƒãƒ¼æ•°", len(mf_model.member_index))

    with col3:
        st.metric("åŠ›é‡æ•°", len(mf_model.competence_index))

    with col4:
        error = mf_model.get_reconstruction_error()
        st.metric("å†æ§‹æˆèª¤å·®", f"{error:.4f}")

    # NMFæˆåˆ†ã®åˆ†æ
    st.markdown("### ğŸ” NMFæ½œåœ¨å› å­ã®åˆ†æ")

    st.markdown(
        "NMFã¯ãƒ¡ãƒ³ãƒãƒ¼Ã—åŠ›é‡ãƒãƒˆãƒªã‚¯ã‚¹ã‚’**ãƒ¡ãƒ³ãƒãƒ¼å› å­è¡Œåˆ—**ã¨**åŠ›é‡å› å­è¡Œåˆ—**ã«åˆ†è§£ã—ã¾ã™ã€‚\n"
        "å„æ½œåœ¨å› å­ã¯ã€ç‰¹å®šã®åŠ›é‡ç¾¤ï¼ˆã‚¹ã‚­ãƒ«ã‚»ãƒƒãƒˆï¼‰ã‚’è¡¨ã—ã€ãƒ¡ãƒ³ãƒãƒ¼ã¯ã“ã‚Œã‚‰ã®å› å­ã®çµ„ã¿åˆã‚ã›ã§è¡¨ç¾ã•ã‚Œã¾ã™ã€‚"
    )

    # å„æ½œåœ¨å› å­ã®ç‰¹å¾´ã‚’åˆ†æ
    with st.expander("ğŸ“ˆ æ½œåœ¨å› å­ã”ã¨ã®ä»£è¡¨åŠ›é‡ï¼ˆãƒˆãƒƒãƒ—10ï¼‰"):
        competence_master = st.session_state.transformed_data["competence_master"]

        n_factors_to_show = min(5, mf_model.n_components)

        for factor_idx in range(n_factors_to_show):
            st.markdown(f"#### æ½œåœ¨å› å­ {factor_idx + 1}")

            # ã“ã®å› å­ã§é‡ã¿ãŒé«˜ã„åŠ›é‡ã‚’å–å¾—
            factor_weights = mf_model.H[factor_idx, :]
            top_indices = factor_weights.argsort()[-10:][::-1]
            top_competences = [mf_model.competence_codes[i] for i in top_indices]
            top_weights = [factor_weights[i] for i in top_indices]

            # åŠ›é‡åã‚’å–å¾—
            top_competence_names = []
            for comp_code in top_competences:
                comp_info = competence_master[competence_master["åŠ›é‡ã‚³ãƒ¼ãƒ‰"] == comp_code]
                if len(comp_info) > 0:
                    top_competence_names.append(comp_info.iloc[0]["åŠ›é‡å"])
                else:
                    top_competence_names.append(comp_code)

            # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã§è¡¨ç¤º
            df_factor = pd.DataFrame({
                "åŠ›é‡å": top_competence_names,
                "é‡ã¿": top_weights
            })

            col1, col2 = st.columns([2, 1])

            with col1:
                # æ£’ã‚°ãƒ©ãƒ•
                fig = px.bar(
                    df_factor,
                    x="é‡ã¿",
                    y="åŠ›é‡å",
                    orientation="h",
                    title=f"æ½œåœ¨å› å­ {factor_idx + 1} ã®ä»£è¡¨åŠ›é‡"
                )
                fig.update_layout(height=400)
                st.plotly_chart(fig, use_container_width=True)

            with col2:
                # ãƒ†ãƒ¼ãƒ–ãƒ«
                st.dataframe(df_factor, use_container_width=True, height=400)

    # ãƒ¡ãƒ³ãƒãƒ¼ã®æ½œåœ¨å› å­åˆ†å¸ƒ
    with st.expander("ğŸ‘¥ ãƒ¡ãƒ³ãƒãƒ¼ã®æ½œåœ¨å› å­åˆ†å¸ƒ"):
        st.markdown("å„ãƒ¡ãƒ³ãƒãƒ¼ãŒã©ã®æ½œåœ¨å› å­ã‚’å¼·ãæŒã£ã¦ã„ã‚‹ã‹ã‚’ç¤ºã—ã¾ã™ã€‚")

        # ãƒ©ãƒ³ãƒ€ãƒ ã«10åã‚’ã‚µãƒ³ãƒ—ãƒ«
        import numpy as np

        n_members_to_show = min(10, len(mf_model.member_codes))
        random_indices = np.random.choice(len(mf_model.member_codes), n_members_to_show, replace=False)

        member_codes = [mf_model.member_codes[i] for i in random_indices]
        member_names = []
        members_df = st.session_state.transformed_data["members_clean"]
        for code in member_codes:
            member_info = members_df[members_df["ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰"] == code]
            if len(member_info) > 0:
                member_names.append(member_info.iloc[0]["ãƒ¡ãƒ³ãƒãƒ¼å"])
            else:
                member_names.append(code)

        # å„ãƒ¡ãƒ³ãƒãƒ¼ã®æ½œåœ¨å› å­ã®é‡ã¿ã‚’å–å¾—ï¼ˆãƒ¡ãƒ³ãƒãƒ¼åã¨ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰ã®ä¸¡æ–¹ã‚’å«ã‚ã‚‹ï¼‰
        member_factors_data = []
        for i, (idx, member_code) in enumerate(zip(random_indices, member_codes)):
            factors = mf_model.W[idx, :]
            for factor_idx, weight in enumerate(factors):
                member_factors_data.append({
                    "ãƒ¡ãƒ³ãƒãƒ¼å": member_names[i],
                    "ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰": member_code,
                    "æ½œåœ¨å› å­": f"å› å­{factor_idx + 1}",
                    "é‡ã¿": weight
                })

        df_member_factors = pd.DataFrame(member_factors_data)

        # ã‚¿ãƒ–ã§2ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ‡ã‚Šæ›¿ãˆ
        tab1, tab2 = st.tabs(["ğŸ“ ãƒ¡ãƒ³ãƒãƒ¼åã§è¡¨ç¤º", "ğŸ”¢ ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰ã§è¡¨ç¤º"])

        with tab1:
            # ãƒ¡ãƒ³ãƒãƒ¼åã§ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
            duplicates = df_member_factors[df_member_factors.duplicated(subset=["ãƒ¡ãƒ³ãƒãƒ¼å", "æ½œåœ¨å› å­"], keep=False)]
            if not duplicates.empty:
                st.warning(f"âš ï¸ é‡è¤‡ãƒ‡ãƒ¼ã‚¿ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸï¼ˆ{len(duplicates)}ä»¶ï¼‰ã€‚é‡è¤‡ã‚’å‰Šé™¤ã—ã¾ã™ã€‚")
                df_member_factors_name = df_member_factors.drop_duplicates(subset=["ãƒ¡ãƒ³ãƒãƒ¼å", "æ½œåœ¨å› å­"], keep="first")
            else:
                df_member_factors_name = df_member_factors.copy()

            pivot_table_name = df_member_factors_name.pivot_table(
                index="ãƒ¡ãƒ³ãƒãƒ¼å",
                columns="æ½œåœ¨å› å­",
                values="é‡ã¿",
                aggfunc="mean"
            )

            fig_name = px.imshow(
                pivot_table_name,
                labels=dict(x="æ½œåœ¨å› å­", y="ãƒ¡ãƒ³ãƒãƒ¼å", color="é‡ã¿"),
                title="ãƒ¡ãƒ³ãƒãƒ¼ã®æ½œåœ¨å› å­åˆ†å¸ƒãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼ˆãƒ¡ãƒ³ãƒãƒ¼åï¼‰",
                color_continuous_scale="Blues"
            )
            fig_name.update_layout(height=500)
            st.plotly_chart(fig_name, use_container_width=True)

        with tab2:
            # ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰ã§ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
            duplicates_code = df_member_factors[df_member_factors.duplicated(subset=["ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰", "æ½œåœ¨å› å­"], keep=False)]
            if not duplicates_code.empty:
                st.warning(f"âš ï¸ é‡è¤‡ãƒ‡ãƒ¼ã‚¿ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸï¼ˆ{len(duplicates_code)}ä»¶ï¼‰ã€‚é‡è¤‡ã‚’å‰Šé™¤ã—ã¾ã™ã€‚")
                df_member_factors_code = df_member_factors.drop_duplicates(subset=["ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰", "æ½œåœ¨å› å­"], keep="first")
            else:
                df_member_factors_code = df_member_factors.copy()

            # ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰ã‚’æ–‡å­—åˆ—å‹ã¨ã—ã¦æ˜ç¤ºçš„ã«å¤‰æ›
            df_member_factors_code["ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰"] = df_member_factors_code["ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰"].astype(str)

            pivot_table_code = df_member_factors_code.pivot_table(
                index="ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰",
                columns="æ½œåœ¨å› å­",
                values="é‡ã¿",
                aggfunc="mean"
            )

            # go.Heatmapã‚’ä½¿ç”¨ã—ã¦ã‚«ãƒ†ã‚´ãƒªãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦æ‰±ã†
            import plotly.graph_objects as go

            fig_code = go.Figure(data=go.Heatmap(
                z=pivot_table_code.values,
                x=pivot_table_code.columns.tolist(),
                y=pivot_table_code.index.tolist(),
                colorscale="Blues",
                colorbar=dict(title="é‡ã¿"),
                hoverongaps=False,
                hovertemplate="ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰: %{y}<br>æ½œåœ¨å› å­: %{x}<br>é‡ã¿: %{z:.3f}<extra></extra>"
            ))

            fig_code.update_layout(
                title="ãƒ¡ãƒ³ãƒãƒ¼ã®æ½œåœ¨å› å­åˆ†å¸ƒãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼ˆãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰ï¼‰",
                xaxis_title="æ½œåœ¨å› å­",
                yaxis_title="ãƒ¡ãƒ³ãƒãƒ¼ã‚³ãƒ¼ãƒ‰",
                height=500,
                yaxis=dict(type='category')  # ã‚«ãƒ†ã‚´ãƒªãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦æ‰±ã†
            )
            st.plotly_chart(fig_code, use_container_width=True)

    # åŠ›é‡ã®æ½œåœ¨å› å­åˆ†å¸ƒ
    with st.expander("ğŸ’¡ åŠ›é‡ã®æ½œåœ¨å› å­åˆ†å¸ƒ"):
        st.markdown("å„åŠ›é‡ãŒã©ã®æ½œåœ¨å› å­ã«é–¢é€£ã—ã¦ã„ã‚‹ã‹ã‚’ç¤ºã—ã¾ã™ã€‚")

        # ãƒ©ãƒ³ãƒ€ãƒ ã«10å€‹ã®åŠ›é‡ã‚’ã‚µãƒ³ãƒ—ãƒ«
        n_competences_to_show = min(10, len(mf_model.competence_codes))
        random_comp_indices = np.random.choice(len(mf_model.competence_codes), n_competences_to_show, replace=False)

        competence_codes = [mf_model.competence_codes[i] for i in random_comp_indices]
        competence_names = []
        for code in competence_codes:
            comp_info = competence_master[competence_master["åŠ›é‡ã‚³ãƒ¼ãƒ‰"] == code]
            if len(comp_info) > 0:
                competence_names.append(comp_info.iloc[0]["åŠ›é‡å"])
            else:
                competence_names.append(code)

        # å„åŠ›é‡ã®æ½œåœ¨å› å­ã®é‡ã¿ã‚’å–å¾—
        competence_factors_data = []
        for i, (idx, comp_code) in enumerate(zip(random_comp_indices, competence_codes)):
            factors = mf_model.H[:, idx]
            for factor_idx, weight in enumerate(factors):
                competence_factors_data.append({
                    "åŠ›é‡": competence_names[i],  # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã§ç›´æ¥å‚ç…§
                    "æ½œåœ¨å› å­": f"å› å­{factor_idx + 1}",
                    "é‡ã¿": weight
                })

        df_competence_factors = pd.DataFrame(competence_factors_data)

        # é‡è¤‡ãƒã‚§ãƒƒã‚¯ã¨ãƒ‡ãƒãƒƒã‚°æƒ…å ±
        duplicates_comp = df_competence_factors[df_competence_factors.duplicated(subset=["åŠ›é‡", "æ½œåœ¨å› å­"], keep=False)]
        if not duplicates_comp.empty:
            st.warning(f"âš ï¸ é‡è¤‡ãƒ‡ãƒ¼ã‚¿ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸï¼ˆ{len(duplicates_comp)}ä»¶ï¼‰ã€‚é‡è¤‡ã‚’å‰Šé™¤ã—ã¾ã™ã€‚")
            df_competence_factors = df_competence_factors.drop_duplicates(subset=["åŠ›é‡", "æ½œåœ¨å› å­"], keep="first")

        # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
        pivot_table_comp = df_competence_factors.pivot_table(
            index="åŠ›é‡",
            columns="æ½œåœ¨å› å­",
            values="é‡ã¿",
            aggfunc="mean"  # ä¸‡ãŒä¸€é‡è¤‡ãŒã‚ã‚‹å ´åˆã¯å¹³å‡ã‚’å–ã‚‹
        )

        fig = px.imshow(
            pivot_table_comp,
            labels=dict(x="æ½œåœ¨å› å­", y="åŠ›é‡", color="é‡ã¿"),
            title="åŠ›é‡ã®æ½œåœ¨å› å­åˆ†å¸ƒãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—",
            color_continuous_scale="Greens"
        )
        fig.update_layout(height=500)
        st.plotly_chart(fig, use_container_width=True)

    # ãƒ¢ãƒ‡ãƒ«è©•ä¾¡æŒ‡æ¨™
    with st.expander("ğŸ“‰ ãƒ¢ãƒ‡ãƒ«è©•ä¾¡æŒ‡æ¨™"):
        st.markdown("### å†æ§‹æˆèª¤å·®ã®è©³ç´°")

        error = mf_model.get_reconstruction_error()

        st.metric("å†æ§‹æˆèª¤å·®ï¼ˆFrobenius ãƒãƒ«ãƒ ï¼‰", f"{error:.6f}")

        # è©•ä¾¡åŸºæº–ã¨æ”¹å–„ææ¡ˆ
        if error < 0.1:
            st.success("âœ… **éå¸¸ã«è‰¯å¥½ãªãƒ¢ãƒ‡ãƒ«ã§ã™**")
            st.markdown("å†æ§‹æˆèª¤å·®ãŒ0.1ä»¥ä¸‹ã§ã€ãƒ¢ãƒ‡ãƒ«ã¯å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚’éå¸¸ã«ã‚ˆãå†ç¾ã—ã¦ã„ã¾ã™ã€‚")
        elif error < 0.3:
            st.success("âœ… **è‰¯å¥½ãªãƒ¢ãƒ‡ãƒ«ã§ã™**")
            st.markdown("å†æ§‹æˆèª¤å·®ãŒ0.3ä»¥ä¸‹ã§ã€ãƒ¢ãƒ‡ãƒ«ã¯å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚ˆãå†ç¾ã—ã¦ã„ã¾ã™ã€‚")
        elif error < 0.5:
            st.warning("âš ï¸ **è¨±å®¹ç¯„å›²ã§ã™ãŒã€æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™**")
            st.markdown("å†æ§‹æˆèª¤å·®ãŒ0.5ä»¥ä¸‹ã§è¨±å®¹ç¯„å›²å†…ã§ã™ãŒã€ã•ã‚‰ãªã‚‹æ”¹å–„ãŒå¯èƒ½ã§ã™ã€‚")
        else:
            st.error("âŒ **æ”¹å–„ãŒå¿…è¦ã§ã™**")
            st.markdown("å†æ§‹æˆèª¤å·®ãŒ0.5ä»¥ä¸Šã§ã€ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦å‘ä¸ŠãŒæ¨å¥¨ã•ã‚Œã¾ã™ã€‚")

        # æ”¹å–„ææ¡ˆï¼ˆèª¤å·®ãŒ0.3ä»¥ä¸Šã®å ´åˆï¼‰
        if error >= 0.3:
            st.markdown("---")
            st.markdown("### ğŸ’¡ æ”¹å–„ææ¡ˆ")

            current_components = mf_model.n_components

            st.info(f"""
            **æ¨å¥¨ã•ã‚Œã‚‹æ”¹å–„ç­–:**

            1. **ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°**:
               - ä¸Šè¨˜ã®ã€Œå­¦ç¿’ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€ã§ã€Œãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° (Optuna)ã€ã‚’æœ‰åŠ¹ã«ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’å†å­¦ç¿’ã—ã¦ãã ã•ã„
               - ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã«ã‚ˆã‚Šæœ€é©ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒè‡ªå‹•çš„ã«æ¢ç´¢ã•ã‚Œã¾ã™

            2. **ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã®æœ‰åŠ¹åŒ–**:
               - ã€Œãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‚’ä½¿ç”¨ã€ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã“ã¨ã§ã€å¤–ã‚Œå€¤ã®é™¤å»ã¨æ­£è¦åŒ–ãŒè¡Œã‚ã‚Œã¾ã™
               - ã‚¹ãƒ‘ãƒ¼ã‚¹ãªãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ç‰¹ã«åŠ¹æœçš„ã§ã™

            3. **æ‰‹å‹•ã§ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´** (config.py):
               - æ½œåœ¨å› å­æ•°: ç¾åœ¨ {current_components} â†’ 25ã€œ35 ã«å¢—åŠ ã‚’æ¤œè¨
               - æ­£å‰‡åŒ–å¼·åº¦: alpha_W, alpha_H ã‚’ 0.05ã€œ0.1 ã«èª¿æ•´
               - æœ€å¤§ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°: max_iter ã‚’ 1500ã€œ2000 ã«å¢—åŠ 

            è©³ç´°ã¯ `docs/NMF_RECONSTRUCTION_ERROR_IMPROVEMENTS.md` ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚
            """)

        # è¿½åŠ ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        st.markdown("---")
        st.markdown("### ğŸ“Š è¿½åŠ ãƒ¡ãƒˆãƒªã‚¯ã‚¹")

        col1, col2, col3 = st.columns(3)

        with col1:
            st.metric("ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°", mf_model.model.n_iter_)

        with col2:
            sparsity_W = np.sum(mf_model.W == 0) / mf_model.W.size * 100
            st.metric("ãƒ¡ãƒ³ãƒãƒ¼å› å­ã®ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§", f"{sparsity_W:.2f}%")

        with col3:
            sparsity_H = np.sum(mf_model.H == 0) / mf_model.H.size * 100
            st.metric("åŠ›é‡å› å­ã®ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§", f"{sparsity_H:.2f}%")

    st.markdown("---")
    st.success("âœ… å­¦ç¿’çµæœã®åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    st.info("ğŸ‘‰ ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ã€Œæ¨è«–ã€ãƒšãƒ¼ã‚¸ã«ç§»å‹•ã—ã¦ã€æ¨è–¦ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
